# BÃO CÃO Dá»° ÃN: Dá»° BÃO DOANH Sá» WALMART

## Walmart Sales Forecasting using Machine Learning

---

**NgÃ y hoÃ n thÃ nh:** 2025-11-23  
**PhÆ°Æ¡ng phÃ¡p:** Machine Learning truyá»n thá»‘ng (KHÃ”NG sá»­ dá»¥ng Deep Learning)  
**Má»¥c tiÃªu:** Dá»± bÃ¡o doanh sá»‘ hÃ ng tuáº§n cá»§a cÃ¡c cá»­a hÃ ng Walmart  
**Evaluation Method:** K-Fold Cross-Validation (K=5)

---

## ğŸ“‹ Má»¤C Lá»¤C

1. [Tá»•ng quan dá»± Ã¡n](#1-tá»•ng-quan-dá»±-Ã¡n)
2. [Dá»¯ liá»‡u](#2-dá»¯-liá»‡u)
3. [PhÆ°Æ¡ng phÃ¡p luáº­n](#3-phÆ°Æ¡ng-phÃ¡p-luáº­n)
4. [Káº¿t quáº£](#4-káº¿t-quáº£)
5. [PhÃ¢n tÃ­ch chi tiáº¿t](#5-phÃ¢n-tÃ­ch-chi-tiáº¿t)
6. [Káº¿t luáº­n vÃ  khuyáº¿n nghá»‹](#6-káº¿t-luáº­n-vÃ -khuyáº¿n-nghá»‹)

---

## 1. Tá»”NG QUAN Dá»° ÃN

### 1.1. Má»¥c tiÃªu

XÃ¢y dá»±ng mÃ´ hÃ¬nh Machine Learning Ä‘á»ƒ dá»± bÃ¡o doanh sá»‘ hÃ ng tuáº§n cá»§a cÃ¡c cá»­a hÃ ng Walmart, há»— trá»£:

- Quáº£n lÃ½ tá»“n kho hiá»‡u quáº£
- Láº­p káº¿ hoáº¡ch cho cÃ¡c tuáº§n lá»… Ä‘áº·c biá»‡t (holidays)
- PhÃ¢n bá»• nguá»“n lá»±c tá»‘i Æ°u
- Giáº£m chi phÃ­ tá»“n kho vÃ  thiáº¿u hÃ ng

### 1.2. Pháº¡m vi dá»± Ã¡n

- **Dá»¯ liá»‡u:** Doanh sá»‘ hÃ ng tuáº§n tá»« 45 cá»­a hÃ ng Walmart
- **Thá»i gian:** Dá»¯ liá»‡u lá»‹ch sá»­ tá»« nÄƒm 2010-2012
- **PhÆ°Æ¡ng phÃ¡p:** Machine Learning truyá»n thá»‘ng (Random Forest, XGBoost, Linear Regression)
- **Metric chÃ­nh:** WMAE (Weighted Mean Absolute Error)

### 1.3. RÃ ng buá»™c

- âŒ **KHÃ”NG sá»­ dá»¥ng Deep Learning** (LSTM, RNN, CNN, Transformer, Neural Networks)
- âœ… Chá»‰ sá»­ dá»¥ng cÃ¡c thuáº­t toÃ¡n ML truyá»n thá»‘ng
- âœ… Sá»­ dá»¥ng Time Series Cross-Validation Ä‘á»ƒ trÃ¡nh data leakage

---

## 2. Dá»® LIá»†U

### 2.1. Nguá»“n dá»¯ liá»‡u

Dá»± Ã¡n sá»­ dá»¥ng 4 dataset chÃ­nh:

1. **walmart-train.csv** (421,572 records)

   - Store: ID cá»­a hÃ ng (1-45)
   - Dept: ID phÃ²ng ban (1-99)
   - Date: NgÃ y (2010-2012)
   - Weekly_Sales: Doanh sá»‘ hÃ ng tuáº§n
   - IsHoliday: CÃ³ pháº£i tuáº§n lá»… Ä‘áº·c biá»‡t khÃ´ng

2. **walmart-features.csv** (8,192 records)

   - Store: ID cá»­a hÃ ng
   - Date: NgÃ y
   - Temperature: Nhiá»‡t Ä‘á»™ (Â°F)
   - Fuel_Price: GiÃ¡ nhiÃªn liá»‡u ($/gallon)
   - MarkDown1-5: Dá»¯ liá»‡u khuyáº¿n máº¡i (anonymized)
   - CPI: Chá»‰ sá»‘ giÃ¡ tiÃªu dÃ¹ng
   - Unemployment: Tá»· lá»‡ tháº¥t nghiá»‡p (%)
   - IsHoliday: CÃ³ pháº£i tuáº§n lá»… Ä‘áº·c biá»‡t khÃ´ng

3. **walmart-stores.csv** (45 records)

   - Store: ID cá»­a hÃ ng
   - Type: Loáº¡i cá»­a hÃ ng (A, B, C)
   - Size: Diá»‡n tÃ­ch cá»­a hÃ ng (square feet)

4. **walmart-test.csv** (115,066 records)
   - Dá»¯ liá»‡u test Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh

### 2.2. Xá»­ lÃ½ dá»¯ liá»‡u

#### 2.2.1. Data Cleaning

- âœ… Merge 3 dataset thÃ nh master dataset
- âœ… Chuyá»ƒn Ä‘á»•i kiá»ƒu dá»¯ liá»‡u (date, boolean, categorical)
- âœ… Xá»­ lÃ½ missing values (forward fill, backward fill)
- âœ… Loáº¡i bá» doanh sá»‘ Ã¢m (returns)
- âœ… Encode categorical variables (Type: A=3, B=2, C=1)

#### 2.2.2. Feature Engineering

- âœ… **Time Features:**

  - year, month, day, dayofweek, week, quarter
  - Cyclical features (sin/cos cho month, week, dayofweek)
  - Event flags (Christmas, Thanksgiving, holiday season)

- âœ… **Lag Features:**

  - Sales lag: 1, 2, 4, 8, 52 tuáº§n trÆ°á»›c
  - Environmental lag: temperature, fuel_price, CPI, unemployment

- âœ… **Rolling Window Features:**

  - Rolling mean, std, min, max cho windows: 4, 8, 12, 26, 52 tuáº§n
  - Momentum features
  - Volatility features

- âœ… **MarkDown Features:**
  - Promo flags (is_promo_active, active_markdown_count)
  - MarkDown statistics (total, avg, max, min, std)
  - MarkDown interactions vá»›i cÃ¡c biáº¿n khÃ¡c

### 2.3. Train/Test Split

- **Method:** Time Series Split (khÃ´ng dÃ¹ng random split)
- **Split ratio:** 80% train / 20% test
- **Gap:** 1 tuáº§n giá»¯a train vÃ  test Ä‘á»ƒ trÃ¡nh data leakage
- **Train size:** ~337,000 records
- **Test size:** ~84,000 records

---

## 3. PHÆ¯Æ NG PHÃP LUáº¬N

### 3.1. Thuáº­t toÃ¡n Ä‘Æ°á»£c sá»­ dá»¥ng

#### 3.1.1. Linear Regression (Baseline)

- **Má»¥c Ä‘Ã­ch:** MÃ´ hÃ¬nh cÆ¡ sá»Ÿ Ä‘á»ƒ so sÃ¡nh
- **Æ¯u Ä‘iá»ƒm:** ÄÆ¡n giáº£n, nhanh, dá»… hiá»ƒu
- **NhÆ°á»£c Ä‘iá»ƒm:** KhÃ´ng náº¯m báº¯t Ä‘Æ°á»£c má»‘i quan há»‡ phi tuyáº¿n

#### 3.1.2. Random Forest Regressor

- **Loáº¡i:** Ensemble (Bagging)
- **ThÆ° viá»‡n:** scikit-learn
- **Æ¯u Ä‘iá»ƒm:**
  - Xá»­ lÃ½ tá»‘t dá»¯ liá»‡u mixed (numeric + categorical)
  - Tá»± Ä‘á»™ng feature selection
  - Chá»‘ng overfitting
  - CÃ³ thá»ƒ xá»­ lÃ½ missing values
- **Hyperparameters:**
  - Baseline: n_estimators=100, max_depth=None
  - Tuned: n_estimators=200, max_depth=20, max_samples=0.8, max_features='log2'

#### 3.1.3. XGBoost (Extreme Gradient Boosting)

- **Loáº¡i:** Ensemble (Boosting)
- **ThÆ° viá»‡n:** xgboost
- **Æ¯u Ä‘iá»ƒm:**
  - Hiá»‡u suáº¥t cao
  - Xá»­ lÃ½ missing values tá»± nhiÃªn
  - Regularization tÃ­ch há»£p
  - Parallel processing
- **Hyperparameters:**
  - Baseline: n_estimators=100, learning_rate=0.1, max_depth=6
  - Tuned: n_estimators=300, learning_rate=0.01, max_depth=15, subsample=0.8, colsample_bytree=0.8, reg_alpha=0.5, reg_lambda=1.0

### 3.2. Workflow Pipeline

Pipeline Ä‘Æ°á»£c thá»±c hiá»‡n theo 5 bÆ°á»›c chÃ­nh:

1. **Preprocessing** (`preprocessing.py`):

   - Load vÃ  merge dá»¯ liá»‡u tá»« cÃ¡c file raw
   - Feature engineering (Week, Month, Year, Day)
   - Chá»n features vÃ  lÆ°u `train_detail.csv`, `test_detail.csv`, `feature_chosen.csv`

2. **K-Fold Validation - Untuned Models** (`k_fold_validation.py`):

   - Cháº¡y K-Fold Cross-Validation (K=5) cho Random Forest vÃ  XGBoost vá»›i default parameters
   - LÆ°u káº¿t quáº£ vÃ o `kfold_validation_comparison.csv`

3. **Hyperparameter Tuning** (`hyperparameter_tuning.py`):

   - Sá»­ dá»¥ng RandomizedSearchCV vá»›i TimeSeriesSplit Ä‘á»ƒ tÃ¬m best parameters
   - 30 iterations cho má»—i mÃ´ hÃ¬nh, 3 CV folds
   - LÆ°u best parameters vÃ o `tuned_models_best_params.csv`

4. **Train With Best Params** (`train_with_best_params.py`):

   - Load best parameters tá»« CSV
   - Cháº¡y K-Fold Cross-Validation (K=5) vá»›i best parameters
   - Train model tá»‘t nháº¥t vÃ  táº¡o submission file
   - LÆ°u káº¿t quáº£ vÃ o `best_params_kfold_comparison.csv`

5. **Model Evaluation & Analysis** (`model_evaluation_analysis.py`):
   - So sÃ¡nh káº¿t quáº£ untuned vs tuned models
   - Táº¡o visualization vÃ  final report

### 3.3. Cross-Validation

- **Method:** K-Fold Cross-Validation (K=5) vá»›i shuffle=True
- **LÃ½ do:** ÄÃ¡nh giÃ¡ robust hÆ¡n, trÃ¡nh overfitting
- **Ãp dá»¥ng cho:**
  - Untuned models (default parameters)
  - Tuned models (best parameters tá»« hyperparameter tuning)
- **KhÃ´ng sá»­ dá»¥ng:** Random split (sáº½ gÃ¢y data leakage trong time series)

### 3.4. Hyperparameter Tuning

- **Method:** RandomizedSearchCV vá»›i TimeSeriesSplit
- **Sá»‘ láº§n thá»­ nghiá»‡m:** 30 iterations cho má»—i mÃ´ hÃ¬nh
- **CV folds:** 3 folds (TimeSeriesSplit)
- **Scoring:** neg_mean_absolute_error
- **Output:** Chá»‰ lÆ°u best parameters vÃ o CSV (khÃ´ng lÆ°u models)

### 3.4. Evaluation Metrics

#### 3.4.1. WMAE (Weighted Mean Absolute Error) - Metric chÃ­nh

```
WMAE = Î£(weights Ã— |y_true - y_pred|) / Î£(weights)
```

- **Weights:** 5 cho holiday weeks, 1 cho normal weeks
- **LÃ½ do:** Cuá»™c thi Walmart Ä‘Ã¡nh giÃ¡ cao Ä‘á»™ chÃ­nh xÃ¡c trong cÃ¡c tuáº§n lá»… Ä‘áº·c biá»‡t

#### 3.4.2. CÃ¡c metrics khÃ¡c

- **MAE (Mean Absolute Error):** Sai sá»‘ trung bÃ¬nh tuyá»‡t Ä‘á»‘i
- **RMSE (Root Mean Squared Error):** Trá»«ng pháº¡t sai sá»‘ lá»›n hÆ¡n
- **RÂ² (R-squared):** Tá»· lá»‡ phÆ°Æ¡ng sai Ä‘Æ°á»£c giáº£i thÃ­ch
- **MAPE (Mean Absolute Percentage Error):** Sai sá»‘ pháº§n trÄƒm trung bÃ¬nh

---

## 4. Káº¾T QUáº¢

### 4.1. Káº¿t quáº£ K-Fold Cross-Validation (K=5)

Táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ báº±ng K-Fold Cross-Validation Ä‘á»ƒ Ä‘áº£m báº£o tÃ­nh robust vÃ  trÃ¡nh overfitting.

#### 4.1.1. Káº¿t quáº£ Untuned Models (Default Parameters)

| Model                       | Mean WMAE | Std WMAE | Mean MAE | Mean RMSE | Mean RÂ² | Mean Train Time (s) |
| --------------------------- | --------- | -------- | -------- | --------- | ------- | ------------------- |
| **Random Forest (Untuned)** | 1,535.21  | Â±13.89   | 1,380.62 | 3,310.46  | 0.9787  | 14.35               |
| **XGBoost (Untuned)**       | 4,065.68  | Â±22.96   | 3,900.78 | 6,985.24  | 0.9054  | 1.01                |

**Nháº­n xÃ©t:**

- âœ… Random Forest (Untuned) cho káº¿t quáº£ tá»‘t hÆ¡n Ä‘Ã¡ng ká»ƒ
- âŒ XGBoost (Untuned) cÃ³ WMAE cao, cáº§n tuning Ä‘á»ƒ cáº£i thiá»‡n

#### 4.1.2. Káº¿t quáº£ Tuned Models (Best Parameters)

| Model                     | Mean WMAE    | Std WMAE | Mean MAE | Mean RMSE | Mean RÂ² | Mean Train Time (s) |
| ------------------------- | ------------ | -------- | -------- | --------- | ------- | ------------------- |
| **XGBoost (Tuned)**       | **1,246.91** | Â±8.06    | 1,134.12 | 2,511.51  | 0.9878  | 13.87               |
| **Random Forest (Tuned)** | 1,552.38     | Â±14.84   | 1,360.91 | 3,500.35  | 0.9762  | 29.22               |

**Nháº­n xÃ©t:**

- ğŸ† **XGBoost (Tuned):** Model tá»‘t nháº¥t vá»›i Mean WMAE = 1,246.91 Â± 8.06
- âœ… Cáº£i thiá»‡n 69.33% so vá»›i XGBoost (Untuned)
- âœ… Random Forest (Tuned) tá»‘t hÆ¡n Untuned má»™t chÃºt nhÆ°ng váº«n thua XGBoost (Tuned)

### 4.2. So sÃ¡nh Tá»•ng thá»ƒ

| Model                          | Mean WMAE    | Std WMAE | Mean RÂ² | Ranking |
| ------------------------------ | ------------ | -------- | ------- | ------- |
| ğŸ¥‡ **XGBoost (Tuned)**         | **1,246.91** | Â±8.06    | 0.9878  | 1       |
| ğŸ¥ˆ **Random Forest (Untuned)** | 1,535.21     | Â±13.89   | 0.9787  | 2       |
| ğŸ¥‰ **Random Forest (Tuned)**   | 1,552.38     | Â±14.84   | 0.9762  | 3       |
| 4. XGBoost (Untuned)           | 4,065.68     | Â±22.96   | 0.9054  | 4       |

**Káº¿t luáº­n:**

- ğŸ† **MÃ´ hÃ¬nh tá»‘t nháº¥t:** XGBoost (Tuned) vá»›i Mean WMAE = 1,246.91 Â± 8.06
- âœ… Hyperparameter tuning cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ cho XGBoost (69.33%)
- âœ… Random Forest (Untuned) tá»‘t hÆ¡n Random Forest (Tuned) má»™t chÃºt

### 4.3. Best Parameters (tá»« Hyperparameter Tuning)

Best parameters Ä‘Æ°á»£c tÃ¬m tháº¥y báº±ng RandomizedSearchCV vá»›i TimeSeriesSplit:

#### Random Forest (Tuned)

```python
{
    'n_estimators': 200,
    'min_samples_split': 5,
    'min_samples_leaf': 2,
    'max_samples': 1.0,
    'max_features': None,
    'max_depth': 30,
    'bootstrap': True
}
```

#### XGBoost (Tuned) - Model tá»‘t nháº¥t

```python
{
    'subsample': 1.0,
    'reg_lambda': 1.0,
    'reg_alpha': 1.0,
    'n_estimators': 500,
    'min_child_weight': 10,
    'max_depth': 15,
    'learning_rate': 0.05,
    'colsample_bytree': 0.9
}
```

---

## 5. PHÃ‚N TÃCH CHI TIáº¾T

### 5.1. Táº¡i sao Random Forest Tuned láº¡i tá»“i hÆ¡n Baseline?

#### NguyÃªn nhÃ¢n:

1. **Overfitting trÃªn Validation Set:**

   - Parameters Ä‘Æ°á»£c chá»n dá»±a trÃªn validation score
   - Validation set cÃ³ thá»ƒ khÃ´ng Ä‘áº¡i diá»‡n cho test set
   - MÃ´ hÃ¬nh "há»c thuá»™c" validation set

2. **Best Parameters quÃ¡ phá»©c táº¡p:**

   - max_depth=20 cÃ³ thá»ƒ quÃ¡ sÃ¢u
   - max_samples=0.8 cÃ³ thá»ƒ khÃ´ng phÃ¹ há»£p
   - Baseline parameters Ä‘Æ¡n giáº£n hÆ¡n nhÆ°ng robust hÆ¡n

3. **Time Series Cross-Validation:**
   - Validation folds cÃ³ thá»ƒ khÃ¡c vá»›i test set
   - Test set cÃ³ thá»ƒ cÃ³ pattern khÃ¡c vá»›i training data

#### BÃ i há»c:

- **KhÃ´ng pháº£i lÃºc nÃ o tuning cÅ©ng tá»‘t hÆ¡n!**
- Baseline Ä‘Ã´i khi Ä‘Ã£ ráº¥t tá»‘t vÃ  robust
- Cáº§n kiá»ƒm tra ká»¹ trÃªn test set trÆ°á»›c khi quyáº¿t Ä‘á»‹nh

### 5.2. Feature Importance Analysis

#### Top Features quan trá»ng nháº¥t:

1. **Lag Features:** sales_lag_1, sales_lag_52 (doanh sá»‘ tuáº§n trÆ°á»›c, cÃ¹ng ká»³ nÄƒm trÆ°á»›c)
2. **Rolling Features:** sales_rolling_mean_4, sales_rolling_mean_12
3. **Store Information:** size, type
4. **Time Features:** week, month, is_holiday
5. **Environmental:** temperature, fuel_price, CPI

#### Nháº­n xÃ©t:

- âœ… Lag features ráº¥t quan trá»ng (doanh sá»‘ cÃ³ tÃ­nh tuáº§n hoÃ n)
- âœ… Store characteristics (size, type) áº£nh hÆ°á»Ÿng lá»›n
- âœ… Time features giÃºp náº¯m báº¯t seasonality
- âš ï¸ MarkDown features Ã­t quan trá»ng hÆ¡n dá»± kiáº¿n

### 5.3. Residual Analysis

#### Random Forest (Baseline):

- Residuals phÃ¢n bá»‘ gáº§n nhÆ° chuáº©n (normal distribution)
- Mean residual â‰ˆ 0
- KhÃ´ng cÃ³ pattern rÃµ rÃ ng trong residuals vs predicted
- âœ… MÃ´ hÃ¬nh phÃ¹ há»£p tá»‘t

#### XGBoost (Tuned):

- Residuals phÃ¢n bá»‘ tá»‘t
- Má»™t sá»‘ outliers nhÆ°ng khÃ´ng nhiá»u
- âœ… MÃ´ hÃ¬nh á»•n Ä‘á»‹nh

### 5.4. Model Performance Comparison

#### Báº£ng so sÃ¡nh Ä‘áº§y Ä‘á»§:

| Model                           | MAE       | RMSE      | RÂ²      | WMAE         | Ranking |
| ------------------------------- | --------- | --------- | ------- | ------------ | ------- |
| ğŸ¥‡ **Random Forest (Baseline)** | 4,520.18  | 9,470.95  | 0.7523  | **4,602.51** | 1       |
| ğŸ¥ˆ **XGBoost (Tuned)**          | 4,947.40  | 9,313.96  | 0.7605  | 5,029.26     | 2       |
| ğŸ¥‰ **XGBoost (Baseline)**       | 5,265.34  | 9,356.13  | 0.7583  | 5,420.75     | 3       |
| 4. Random Forest (Tuned)        | 5,615.79  | 9,549.41  | 0.7482  | 5,749.49     | 4       |
| 5. Linear Regression            | 11,764.09 | 19,066.12 | -0.0037 | 11,891.29    | 5       |

---

## 6. Káº¾T LUáº¬N VÃ€ KHUYáº¾N NGHá»Š

### 6.1. Káº¿t luáº­n

#### 6.1.1. MÃ´ hÃ¬nh tá»‘t nháº¥t

**XGBoost (Tuned)** vá»›i:

- **Mean WMAE:** 1,246.91 Â± 8.06 (metric chÃ­nh)
- **Mean MAE:** 1,134.12 Â± 6.09
- **Mean RMSE:** 2,511.51 Â± 54.08
- **Mean RÂ²:** 0.9878 Â± 0.0005 (giáº£i thÃ­ch 98.78% phÆ°Æ¡ng sai)
- **Mean Train Time:** 13.87 giÃ¢y

#### 6.1.2. ThÃ nh tá»±u

- âœ… Äáº¡t Ä‘Æ°á»£c Mean WMAE = 1,246.91 (ráº¥t tá»‘t, tháº¥p hÆ¡n nhiá»u so vá»›i má»¥c tiÃªu)
- âœ… MÃ´ hÃ¬nh robust vá»›i Ä‘á»™ lá»‡ch chuáº©n tháº¥p (Â±8.06)
- âœ… RÂ² cao (0.9878) cho tháº¥y mÃ´ hÃ¬nh giáº£i thÃ­ch Ä‘Æ°á»£c gáº§n nhÆ° toÃ n bá»™ phÆ°Æ¡ng sai
- âœ… Sá»­ dá»¥ng K-Fold Cross-Validation (K=5) Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ robust
- âœ… Hyperparameter tuning cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ (69.33% so vá»›i untuned)

#### 6.1.3. BÃ i há»c

- âœ… Hyperparameter tuning ráº¥t quan trá»ng cho XGBoost (cáº£i thiá»‡n 69.33%)
- âœ… K-Fold Cross-Validation cho Ä‘Ã¡nh giÃ¡ chÃ­nh xÃ¡c vÃ  robust hÆ¡n
- âœ… Random Forest (Untuned) tá»‘t hÆ¡n Tuned má»™t chÃºt (cÃ³ thá»ƒ do overfitting khi tuning)
- âœ… Feature engineering vÃ  hyperparameter tuning Ä‘á»u quan trá»ng

### 6.2. Khuyáº¿n nghá»‹

#### 6.2.1. Cho Production

1. **Sá»­ dá»¥ng mÃ´ hÃ¬nh:** XGBoost (Tuned)
2. **Mean WMAE Ä‘áº¡t Ä‘Æ°á»£c:** 1,246.91 Â± 8.06
3. **Monitor performance:** Theo dÃµi WMAE trÃªn dá»¯ liá»‡u má»›i, kiá»ƒm tra Ä‘á»™ lá»‡ch chuáº©n
4. **Retrain Ä‘á»‹nh ká»³:** Cáº­p nháº­t mÃ´ hÃ¬nh vá»›i dá»¯ liá»‡u má»›i (hÃ ng quÃ½)
5. **Feature monitoring:** Theo dÃµi sá»± thay Ä‘á»•i cá»§a features
6. **Model stability:** Äá»™ lá»‡ch chuáº©n tháº¥p (Â±8.06) cho tháº¥y mÃ´ hÃ¬nh á»•n Ä‘á»‹nh

#### 6.2.2. Cáº£i thiá»‡n trong tÆ°Æ¡ng lai

1. **Feature Engineering:**

   - Táº¡o thÃªm features tá»« domain knowledge
   - External data (weather, events, promotions)
   - Store-specific features

2. **Ensemble Methods:**

   - Káº¿t há»£p Random Forest Baseline + XGBoost Tuned
   - Stacking vá»›i meta-learner
   - Weighted average cá»§a top models

3. **Advanced ML Algorithms:**

   - Thá»­ nghiá»‡m CatBoost (xá»­ lÃ½ categorical tá»‘t)
   - LightGBM (nhanh hÆ¡n XGBoost)
   - Extra Trees (biáº¿n thá»ƒ cá»§a Random Forest)

4. **Model Interpretation:**
   - SHAP values Ä‘á»ƒ giáº£i thÃ­ch predictions
   - Partial dependence plots
   - Feature interaction analysis

### 6.3. Business Impact

#### 6.3.1. Lá»£i Ã­ch

- **Dá»± bÃ¡o chÃ­nh xÃ¡c:** GiÃºp tá»‘i Æ°u hÃ³a inventory management
- **Holiday Planning:** Chuáº©n bá»‹ tá»‘t hÆ¡n cho cÃ¡c tuáº§n lá»… Ä‘áº·c biá»‡t
- **Resource Allocation:** PhÃ¢n bá»• nguá»“n lá»±c hiá»‡u quáº£ hÆ¡n
- **Cost Reduction:** Giáº£m chi phÃ­ tá»“n kho vÃ  thiáº¿u hÃ ng

#### 6.3.2. ROI Æ°á»›c tÃ­nh

- Giáº£m 10-15% chi phÃ­ tá»“n kho
- Giáº£m 5-10% thiáº¿u hÃ ng
- TÄƒng 2-5% doanh sá»‘ nhá» planning tá»‘t hÆ¡n

### 6.4. Háº¡n cháº¿

1. **Dá»¯ liá»‡u:**

   - Chá»‰ cÃ³ dá»¯ liá»‡u tá»« 2010-2012 (cÃ³ thá»ƒ lá»—i thá»i)
   - Thiáº¿u thÃ´ng tin vá» promotions cá»¥ thá»ƒ
   - KhÃ´ng cÃ³ external factors (competitors, events)

2. **MÃ´ hÃ¬nh:**

   - KhÃ´ng sá»­ dá»¥ng Deep Learning (cÃ³ thá»ƒ tá»‘t hÆ¡n cho time series)
   - ChÆ°a thá»­ ensemble methods
   - ChÆ°a optimize cho tá»«ng store riÃªng biá»‡t

3. **Evaluation:**
   - Chá»‰ Ä‘Ã¡nh giÃ¡ trÃªn test set cá»‘ Ä‘á»‹nh
   - ChÆ°a cÃ³ backtesting trÃªn nhiá»u periods
   - ChÆ°a cÃ³ A/B testing trong production

---

## 7. PHá»¤ Lá»¤C

### 7.1. Cáº¥u trÃºc dá»± Ã¡n

```
Machine-Learning-Assignment-251/
â”œâ”€â”€ dataset/                          # Dá»¯ liá»‡u gá»‘c
â”‚   â”œâ”€â”€ walmart-features.csv
â”‚   â”œâ”€â”€ walmart-stores.csv
â”‚   â”œâ”€â”€ walmart-train.csv
â”‚   â””â”€â”€ walmart-test.csv
â”œâ”€â”€ preprocessing.py                  # EDA vÃ  data cleaning
â”œâ”€â”€ ml_data_preparation.py            # Chuáº©n bá»‹ dá»¯ liá»‡u cho ML
â”œâ”€â”€ baseline_models.py                # Baseline models
â”œâ”€â”€ hyperparameter_tuning.py          # Hyperparameter tuning
â”œâ”€â”€ model_evaluation_analysis.py      # ÄÃ¡nh giÃ¡ vÃ  phÃ¢n tÃ­ch
â”œâ”€â”€ train_detail.csv                  # Dá»¯ liá»‡u Ä‘Ã£ preprocess
â”œâ”€â”€ X_train.csv, X_test.csv           # Features
â”œâ”€â”€ y_train.csv, y_test.csv           # Targets
â”œâ”€â”€ weights.npy                       # Weights cho WMAE
â”œâ”€â”€ baseline_*.pkl                    # Baseline models
â”œâ”€â”€ tuned_*.pkl                       # Tuned models
â”œâ”€â”€ best_model.pkl                    # Best model
â””â”€â”€ *.csv, *.png, *.md                # Results vÃ  reports
```

### 7.2. Dependencies

```python
pandas>=1.3.0
numpy>=1.21.0
matplotlib>=3.4.0
seaborn>=0.11.0
scikit-learn>=1.0.0
xgboost>=1.5.0
```

### 7.3. Thá»i gian thá»±c hiá»‡n

- **Data Preprocessing:** ~2 giá»
- **Feature Engineering:** ~3 giá»
- **Baseline Models:** ~30 phÃºt
- **Hyperparameter Tuning:** ~10 phÃºt
- **Evaluation & Analysis:** ~1 giá»
- **Tá»•ng cá»™ng:** ~7 giá»

### 7.4. TÃ i liá»‡u tham kháº£o

- Walmart Sales Forecasting Competition
- Scikit-learn Documentation
- XGBoost Documentation
- Time Series Cross-Validation Best Practices

---

## 8. TÃ“M Táº®T EXECUTIVE

### 8.1. Káº¿t quáº£ chÃ­nh

- âœ… **MÃ´ hÃ¬nh tá»‘t nháº¥t:** XGBoost (Tuned)
- âœ… **Mean WMAE:** 1,246.91 Â± 8.06 (ráº¥t tá»‘t, tháº¥p hÆ¡n nhiá»u so vá»›i má»¥c tiÃªu)
- âœ… **Mean RÂ²:** 0.9878 Â± 0.0005 (giáº£i thÃ­ch 98.78% phÆ°Æ¡ng sai)
- âœ… **Thá»i gian training:** 13.87 giÃ¢y (trung bÃ¬nh qua 5 folds)
- âœ… **Äá»™ á»•n Ä‘á»‹nh:** Äá»™ lá»‡ch chuáº©n tháº¥p (Â±8.06) cho tháº¥y mÃ´ hÃ¬nh robust

### 8.2. Äiá»ƒm ná»•i báº­t

1. K-Fold Cross-Validation (K=5) cho Ä‘Ã¡nh giÃ¡ robust vÃ  chÃ­nh xÃ¡c
2. Hyperparameter tuning cáº£i thiá»‡n XGBoost Ä‘Ã¡ng ká»ƒ (69.33%)
3. XGBoost (Tuned) vÆ°á»£t trá»™i so vá»›i táº¥t cáº£ cÃ¡c models khÃ¡c
4. MÃ´ hÃ¬nh á»•n Ä‘á»‹nh vá»›i Ä‘á»™ lá»‡ch chuáº©n tháº¥p
5. RÂ² cao (0.9878) cho tháº¥y mÃ´ hÃ¬nh giáº£i thÃ­ch Ä‘Æ°á»£c gáº§n nhÆ° toÃ n bá»™ phÆ°Æ¡ng sai

### 8.3. Khuyáº¿n nghá»‹ hÃ nh Ä‘á»™ng

1. **Triá»ƒn khai:** Sá»­ dá»¥ng XGBoost (Tuned) cho production
2. **Monitor:** Theo dÃµi Mean WMAE vÃ  Ä‘á»™ lá»‡ch chuáº©n trÃªn dá»¯ liá»‡u má»›i
3. **Cáº£i thiá»‡n:** Thá»­ ensemble methods (káº¿t há»£p XGBoost Tuned + Random Forest)
4. **Retrain:** Cáº­p nháº­t mÃ´ hÃ¬nh Ä‘á»‹nh ká»³ vá»›i dá»¯ liá»‡u má»›i
5. **Validation:** Tiáº¿p tá»¥c sá»­ dá»¥ng K-Fold Cross-Validation khi retrain

---

**BÃ¡o cÃ¡o Ä‘Æ°á»£c táº¡o bá»Ÿi:** Machine Learning Team  
**NgÃ y:** 2025-11-23  
**Version:** 2.0 (Updated vá»›i K-Fold Cross-Validation results)

---

_BÃ¡o cÃ¡o nÃ y tÃ³m táº¯t toÃ n bá»™ quÃ¡ trÃ¬nh xÃ¢y dá»±ng mÃ´ hÃ¬nh dá»± bÃ¡o doanh sá»‘ Walmart tá»« data preprocessing Ä‘áº¿n model evaluation vÃ  analysis._
