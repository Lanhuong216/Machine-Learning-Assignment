# B√ÅO C√ÅO D·ª∞ √ÅN: D·ª∞ B√ÅO DOANH S·ªê WALMART
## Walmart Sales Forecasting using Machine Learning

---

**Ng√†y ho√†n th√†nh:** 2024  
**Ph∆∞∆°ng ph√°p:** Machine Learning truy·ªÅn th·ªëng (KH√îNG s·ª≠ d·ª•ng Deep Learning)  
**M·ª•c ti√™u:** D·ª± b√°o doanh s·ªë h√†ng tu·∫ßn c·ªßa c√°c c·ª≠a h√†ng Walmart

---

## üìã M·ª§C L·ª§C

1. [T·ªïng quan d·ª± √°n](#1-t·ªïng-quan-d·ª±-√°n)
2. [D·ªØ li·ªáu](#2-d·ªØ-li·ªáu)
3. [Ph∆∞∆°ng ph√°p lu·∫≠n](#3-ph∆∞∆°ng-ph√°p-lu·∫≠n)
4. [K·∫øt qu·∫£](#4-k·∫øt-qu·∫£)
5. [Ph√¢n t√≠ch chi ti·∫øt](#5-ph√¢n-t√≠ch-chi-ti·∫øt)
6. [K·∫øt lu·∫≠n v√† khuy·∫øn ngh·ªã](#6-k·∫øt-lu·∫≠n-v√†-khuy·∫øn-ngh·ªã)

---

## 1. T·ªîNG QUAN D·ª∞ √ÅN

### 1.1. M·ª•c ti√™u
X√¢y d·ª±ng m√¥ h√¨nh Machine Learning ƒë·ªÉ d·ª± b√°o doanh s·ªë h√†ng tu·∫ßn c·ªßa c√°c c·ª≠a h√†ng Walmart, h·ªó tr·ª£:
- Qu·∫£n l√Ω t·ªìn kho hi·ªáu qu·∫£
- L·∫≠p k·∫ø ho·∫°ch cho c√°c tu·∫ßn l·ªÖ ƒë·∫∑c bi·ªát (holidays)
- Ph√¢n b·ªï ngu·ªìn l·ª±c t·ªëi ∆∞u
- Gi·∫£m chi ph√≠ t·ªìn kho v√† thi·∫øu h√†ng

### 1.2. Ph·∫°m vi d·ª± √°n
- **D·ªØ li·ªáu:** Doanh s·ªë h√†ng tu·∫ßn t·ª´ 45 c·ª≠a h√†ng Walmart
- **Th·ªùi gian:** D·ªØ li·ªáu l·ªãch s·ª≠ t·ª´ nƒÉm 2010-2012
- **Ph∆∞∆°ng ph√°p:** Machine Learning truy·ªÅn th·ªëng (Random Forest, XGBoost, Linear Regression)
- **Metric ch√≠nh:** WMAE (Weighted Mean Absolute Error)

### 1.3. R√†ng bu·ªôc
- ‚ùå **KH√îNG s·ª≠ d·ª•ng Deep Learning** (LSTM, RNN, CNN, Transformer, Neural Networks)
- ‚úÖ Ch·ªâ s·ª≠ d·ª•ng c√°c thu·∫≠t to√°n ML truy·ªÅn th·ªëng
- ‚úÖ S·ª≠ d·ª•ng Time Series Cross-Validation ƒë·ªÉ tr√°nh data leakage

---

## 2. D·ªÆ LI·ªÜU

### 2.1. Ngu·ªìn d·ªØ li·ªáu
D·ª± √°n s·ª≠ d·ª•ng 4 dataset ch√≠nh:

1. **walmart-train.csv** (421,572 records)
   - Store: ID c·ª≠a h√†ng (1-45)
   - Dept: ID ph√≤ng ban (1-99)
   - Date: Ng√†y (2010-2012)
   - Weekly_Sales: Doanh s·ªë h√†ng tu·∫ßn
   - IsHoliday: C√≥ ph·∫£i tu·∫ßn l·ªÖ ƒë·∫∑c bi·ªát kh√¥ng

2. **walmart-features.csv** (8,192 records)
   - Store: ID c·ª≠a h√†ng
   - Date: Ng√†y
   - Temperature: Nhi·ªát ƒë·ªô (¬∞F)
   - Fuel_Price: Gi√° nhi√™n li·ªáu ($/gallon)
   - MarkDown1-5: D·ªØ li·ªáu khuy·∫øn m·∫°i (anonymized)
   - CPI: Ch·ªâ s·ªë gi√° ti√™u d√πng
   - Unemployment: T·ª∑ l·ªá th·∫•t nghi·ªáp (%)
   - IsHoliday: C√≥ ph·∫£i tu·∫ßn l·ªÖ ƒë·∫∑c bi·ªát kh√¥ng

3. **walmart-stores.csv** (45 records)
   - Store: ID c·ª≠a h√†ng
   - Type: Lo·∫°i c·ª≠a h√†ng (A, B, C)
   - Size: Di·ªán t√≠ch c·ª≠a h√†ng (square feet)

4. **walmart-test.csv** (115,066 records)
   - D·ªØ li·ªáu test ƒë·ªÉ ƒë√°nh gi√° m√¥ h√¨nh

### 2.2. X·ª≠ l√Ω d·ªØ li·ªáu

#### 2.2.1. Data Cleaning
- ‚úÖ Merge 3 dataset th√†nh master dataset
- ‚úÖ Chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu (date, boolean, categorical)
- ‚úÖ X·ª≠ l√Ω missing values (forward fill, backward fill)
- ‚úÖ Lo·∫°i b·ªè doanh s·ªë √¢m (returns)
- ‚úÖ Encode categorical variables (Type: A=3, B=2, C=1)

#### 2.2.2. Feature Engineering
- ‚úÖ **Time Features:**
  - year, month, day, dayofweek, week, quarter
  - Cyclical features (sin/cos cho month, week, dayofweek)
  - Event flags (Christmas, Thanksgiving, holiday season)

- ‚úÖ **Lag Features:**
  - Sales lag: 1, 2, 4, 8, 52 tu·∫ßn tr∆∞·ªõc
  - Environmental lag: temperature, fuel_price, CPI, unemployment

- ‚úÖ **Rolling Window Features:**
  - Rolling mean, std, min, max cho windows: 4, 8, 12, 26, 52 tu·∫ßn
  - Momentum features
  - Volatility features

- ‚úÖ **MarkDown Features:**
  - Promo flags (is_promo_active, active_markdown_count)
  - MarkDown statistics (total, avg, max, min, std)
  - MarkDown interactions v·ªõi c√°c bi·∫øn kh√°c

### 2.3. Train/Test Split
- **Method:** Time Series Split (kh√¥ng d√πng random split)
- **Split ratio:** 80% train / 20% test
- **Gap:** 1 tu·∫ßn gi·ªØa train v√† test ƒë·ªÉ tr√°nh data leakage
- **Train size:** ~337,000 records
- **Test size:** ~84,000 records

---

## 3. PH∆Ø∆†NG PH√ÅP LU·∫¨N

### 3.1. Thu·∫≠t to√°n ƒë∆∞·ª£c s·ª≠ d·ª•ng

#### 3.1.1. Linear Regression (Baseline)
- **M·ª•c ƒë√≠ch:** M√¥ h√¨nh c∆° s·ªü ƒë·ªÉ so s√°nh
- **∆Øu ƒëi·ªÉm:** ƒê∆°n gi·∫£n, nhanh, d·ªÖ hi·ªÉu
- **Nh∆∞·ª£c ƒëi·ªÉm:** Kh√¥ng n·∫Øm b·∫Øt ƒë∆∞·ª£c m·ªëi quan h·ªá phi tuy·∫øn

#### 3.1.2. Random Forest Regressor
- **Lo·∫°i:** Ensemble (Bagging)
- **Th∆∞ vi·ªán:** scikit-learn
- **∆Øu ƒëi·ªÉm:**
  - X·ª≠ l√Ω t·ªët d·ªØ li·ªáu mixed (numeric + categorical)
  - T·ª± ƒë·ªông feature selection
  - Ch·ªëng overfitting
  - C√≥ th·ªÉ x·ª≠ l√Ω missing values
- **Hyperparameters:**
  - Baseline: n_estimators=100, max_depth=None
  - Tuned: n_estimators=200, max_depth=20, max_samples=0.8, max_features='log2'

#### 3.1.3. XGBoost (Extreme Gradient Boosting)
- **Lo·∫°i:** Ensemble (Boosting)
- **Th∆∞ vi·ªán:** xgboost
- **∆Øu ƒëi·ªÉm:**
  - Hi·ªáu su·∫•t cao
  - X·ª≠ l√Ω missing values t·ª± nhi√™n
  - Regularization t√≠ch h·ª£p
  - Parallel processing
- **Hyperparameters:**
  - Baseline: n_estimators=100, learning_rate=0.1, max_depth=6
  - Tuned: n_estimators=300, learning_rate=0.01, max_depth=15, subsample=0.8, colsample_bytree=0.8, reg_alpha=0.5, reg_lambda=1.0

### 3.2. Cross-Validation
- **Method:** TimeSeriesSplit (n_splits=3)
- **L√Ω do:** Tr√°nh data leakage trong time series data
- **Kh√¥ng s·ª≠ d·ª•ng:** Random split (s·∫Ω g√¢y data leakage)

### 3.3. Hyperparameter Tuning
- **Method:** RandomizedSearchCV
- **S·ªë l·∫ßn th·ª≠ nghi·ªám:** 30 iterations cho m·ªói m√¥ h√¨nh
- **CV folds:** 3 folds (TimeSeriesSplit)
- **Scoring:** neg_mean_absolute_error
- **Th·ªùi gian tuning:**
  - Random Forest: ~4.76 ph√∫t
  - XGBoost: ~1.62 ph√∫t

### 3.4. Evaluation Metrics

#### 3.4.1. WMAE (Weighted Mean Absolute Error) - Metric ch√≠nh
```
WMAE = Œ£(weights √ó |y_true - y_pred|) / Œ£(weights)
```
- **Weights:** 5 cho holiday weeks, 1 cho normal weeks
- **L√Ω do:** Cu·ªôc thi Walmart ƒë√°nh gi√° cao ƒë·ªô ch√≠nh x√°c trong c√°c tu·∫ßn l·ªÖ ƒë·∫∑c bi·ªát

#### 3.4.2. C√°c metrics kh√°c
- **MAE (Mean Absolute Error):** Sai s·ªë trung b√¨nh tuy·ªát ƒë·ªëi
- **RMSE (Root Mean Squared Error):** Tr·ª´ng ph·∫°t sai s·ªë l·ªõn h∆°n
- **R¬≤ (R-squared):** T·ª∑ l·ªá ph∆∞∆°ng sai ƒë∆∞·ª£c gi·∫£i th√≠ch
- **MAPE (Mean Absolute Percentage Error):** Sai s·ªë ph·∫ßn trƒÉm trung b√¨nh

---

## 4. K·∫æT QU·∫¢

### 4.1. K·∫øt qu·∫£ Baseline Models

| Model | MAE | RMSE | R¬≤ | WMAE | Training Time (s) |
|-------|-----|------|----|----|-------------------|
| **Random Forest** | 4,520.18 | 9,470.95 | 0.7523 | **4,602.51** | 18.99 |
| **XGBoost** | 5,265.34 | 9,356.13 | 0.7583 | 5,420.75 | 0.45 |
| **Linear Regression** | 11,764.09 | 19,066.12 | -0.0037 | 11,891.29 | 0.09 |

**Nh·∫≠n x√©t:**
- ‚úÖ Random Forest cho k·∫øt qu·∫£ t·ªët nh·∫•t (WMAE = 4,602.51)
- ‚úÖ XGBoost c√≥ R¬≤ cao nh·∫•t (0.7583) nh∆∞ng WMAE cao h∆°n
- ‚ùå Linear Regression kh√¥ng ph√π h·ª£p (R¬≤ √¢m, WMAE r·∫•t cao)

### 4.2. K·∫øt qu·∫£ sau Hyperparameter Tuning

| Model | MAE | RMSE | R¬≤ | WMAE | Improvement |
|-------|-----|------|----|----|-------------|
| **Random Forest (Baseline)** | 4,520.18 | 9,470.95 | 0.7523 | **4,602.51** | - |
| **Random Forest (Tuned)** | 5,615.79 | 9,549.41 | 0.7482 | 5,749.49 | ‚ùå -24.92% |
| **XGBoost (Baseline)** | 5,265.34 | 9,356.13 | 0.7583 | 5,420.75 | - |
| **XGBoost (Tuned)** | 4,947.40 | 9,313.96 | 0.7605 | 5,029.26 | ‚úÖ +7.22% |

**Nh·∫≠n x√©t:**
- ‚úÖ **XGBoost Tuned:** C·∫£i thi·ªán 7.22% so v·ªõi baseline
- ‚ùå **Random Forest Tuned:** T·ªìi h∆°n baseline 24.92% (overfitting)
- üèÜ **M√¥ h√¨nh t·ªët nh·∫•t:** Random Forest (Baseline) v·ªõi WMAE = 4,602.51

### 4.3. Best Parameters

#### Random Forest (Tuned)
```python
{
    'n_estimators': 200,
    'max_depth': 20,
    'min_samples_split': 2,
    'min_samples_leaf': 1,
    'max_features': 'log2',
    'max_samples': 0.8,
    'bootstrap': True
}
```

#### XGBoost (Tuned)
```python
{
    'n_estimators': 300,
    'learning_rate': 0.01,
    'max_depth': 15,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'reg_alpha': 0.5,
    'reg_lambda': 1.0,
    'min_child_weight': 1
}
```

---

## 5. PH√ÇN T√çCH CHI TI·∫æT

### 5.1. T·∫°i sao Random Forest Tuned l·∫°i t·ªìi h∆°n Baseline?

#### Nguy√™n nh√¢n:
1. **Overfitting tr√™n Validation Set:**
   - Parameters ƒë∆∞·ª£c ch·ªçn d·ª±a tr√™n validation score
   - Validation set c√≥ th·ªÉ kh√¥ng ƒë·∫°i di·ªán cho test set
   - M√¥ h√¨nh "h·ªçc thu·ªôc" validation set

2. **Best Parameters qu√° ph·ª©c t·∫°p:**
   - max_depth=20 c√≥ th·ªÉ qu√° s√¢u
   - max_samples=0.8 c√≥ th·ªÉ kh√¥ng ph√π h·ª£p
   - Baseline parameters ƒë∆°n gi·∫£n h∆°n nh∆∞ng robust h∆°n

3. **Time Series Cross-Validation:**
   - Validation folds c√≥ th·ªÉ kh√°c v·ªõi test set
   - Test set c√≥ th·ªÉ c√≥ pattern kh√°c v·ªõi training data

#### B√†i h·ªçc:
- **Kh√¥ng ph·∫£i l√∫c n√†o tuning c≈©ng t·ªët h∆°n!**
- Baseline ƒë√¥i khi ƒë√£ r·∫•t t·ªët v√† robust
- C·∫ßn ki·ªÉm tra k·ªπ tr√™n test set tr∆∞·ªõc khi quy·∫øt ƒë·ªãnh

### 5.2. Feature Importance Analysis

#### Top Features quan tr·ªçng nh·∫•t:
1. **Lag Features:** sales_lag_1, sales_lag_52 (doanh s·ªë tu·∫ßn tr∆∞·ªõc, c√πng k·ª≥ nƒÉm tr∆∞·ªõc)
2. **Rolling Features:** sales_rolling_mean_4, sales_rolling_mean_12
3. **Store Information:** size, type
4. **Time Features:** week, month, is_holiday
5. **Environmental:** temperature, fuel_price, CPI

#### Nh·∫≠n x√©t:
- ‚úÖ Lag features r·∫•t quan tr·ªçng (doanh s·ªë c√≥ t√≠nh tu·∫ßn ho√†n)
- ‚úÖ Store characteristics (size, type) ·∫£nh h∆∞·ªüng l·ªõn
- ‚úÖ Time features gi√∫p n·∫Øm b·∫Øt seasonality
- ‚ö†Ô∏è MarkDown features √≠t quan tr·ªçng h∆°n d·ª± ki·∫øn

### 5.3. Residual Analysis

#### Random Forest (Baseline):
- Residuals ph√¢n b·ªë g·∫ßn nh∆∞ chu·∫©n (normal distribution)
- Mean residual ‚âà 0
- Kh√¥ng c√≥ pattern r√µ r√†ng trong residuals vs predicted
- ‚úÖ M√¥ h√¨nh ph√π h·ª£p t·ªët

#### XGBoost (Tuned):
- Residuals ph√¢n b·ªë t·ªët
- M·ªôt s·ªë outliers nh∆∞ng kh√¥ng nhi·ªÅu
- ‚úÖ M√¥ h√¨nh ·ªïn ƒë·ªãnh

### 5.4. Model Performance Comparison

#### B·∫£ng so s√°nh ƒë·∫ßy ƒë·ªß:

| Model | MAE | RMSE | R¬≤ | WMAE | Ranking |
|-------|-----|------|----|----|---------|
| ü•á **Random Forest (Baseline)** | 4,520.18 | 9,470.95 | 0.7523 | **4,602.51** | 1 |
| ü•à **XGBoost (Tuned)** | 4,947.40 | 9,313.96 | 0.7605 | 5,029.26 | 2 |
| ü•â **XGBoost (Baseline)** | 5,265.34 | 9,356.13 | 0.7583 | 5,420.75 | 3 |
| 4. Random Forest (Tuned) | 5,615.79 | 9,549.41 | 0.7482 | 5,749.49 | 4 |
| 5. Linear Regression | 11,764.09 | 19,066.12 | -0.0037 | 11,891.29 | 5 |

---

## 6. K·∫æT LU·∫¨N V√Ä KHUY·∫æN NGH·ªä

### 6.1. K·∫øt lu·∫≠n

#### 6.1.1. M√¥ h√¨nh t·ªët nh·∫•t
**Random Forest (Baseline)** v·ªõi:
- **WMAE:** 4,602.51 (metric ch√≠nh)
- **MAE:** 4,520.18
- **RMSE:** 9,470.95
- **R¬≤:** 0.7523 (gi·∫£i th√≠ch 75.23% ph∆∞∆°ng sai)

#### 6.1.2. Th√†nh t·ª±u
- ‚úÖ ƒê·∫°t ƒë∆∞·ª£c WMAE < 5,000 (m·ª•c ti√™u)
- ‚úÖ M√¥ h√¨nh robust, kh√¥ng overfitting
- ‚úÖ Feature engineering hi·ªáu qu·∫£
- ‚úÖ S·ª≠ d·ª•ng ƒë√∫ng Time Series Cross-Validation

#### 6.1.3. B√†i h·ªçc
- ‚úÖ Baseline ƒë√¥i khi t·ªët h∆°n tuned model
- ‚úÖ C·∫ßn ki·ªÉm tra k·ªπ tr√™n test set
- ‚úÖ Feature engineering quan tr·ªçng h∆°n hyperparameter tuning
- ‚úÖ Time series data c·∫ßn x·ª≠ l√Ω ƒë·∫∑c bi·ªát

### 6.2. Khuy·∫øn ngh·ªã

#### 6.2.1. Cho Production
1. **S·ª≠ d·ª•ng m√¥ h√¨nh:** Random Forest (Baseline)
2. **WMAE ƒë·∫°t ƒë∆∞·ª£c:** 4,602.51
3. **Monitor performance:** Theo d√µi WMAE tr√™n d·ªØ li·ªáu m·ªõi
4. **Retrain ƒë·ªãnh k·ª≥:** C·∫≠p nh·∫≠t m√¥ h√¨nh v·ªõi d·ªØ li·ªáu m·ªõi (h√†ng qu√Ω)
5. **Feature monitoring:** Theo d√µi s·ª± thay ƒë·ªïi c·ªßa features

#### 6.2.2. C·∫£i thi·ªán trong t∆∞∆°ng lai
1. **Feature Engineering:**
   - T·∫°o th√™m features t·ª´ domain knowledge
   - External data (weather, events, promotions)
   - Store-specific features

2. **Ensemble Methods:**
   - K·∫øt h·ª£p Random Forest Baseline + XGBoost Tuned
   - Stacking v·ªõi meta-learner
   - Weighted average c·ªßa top models

3. **Advanced ML Algorithms:**
   - Th·ª≠ nghi·ªám CatBoost (x·ª≠ l√Ω categorical t·ªët)
   - LightGBM (nhanh h∆°n XGBoost)
   - Extra Trees (bi·∫øn th·ªÉ c·ªßa Random Forest)

4. **Model Interpretation:**
   - SHAP values ƒë·ªÉ gi·∫£i th√≠ch predictions
   - Partial dependence plots
   - Feature interaction analysis

### 6.3. Business Impact

#### 6.3.1. L·ª£i √≠ch
- **D·ª± b√°o ch√≠nh x√°c:** Gi√∫p t·ªëi ∆∞u h√≥a inventory management
- **Holiday Planning:** Chu·∫©n b·ªã t·ªët h∆°n cho c√°c tu·∫ßn l·ªÖ ƒë·∫∑c bi·ªát
- **Resource Allocation:** Ph√¢n b·ªï ngu·ªìn l·ª±c hi·ªáu qu·∫£ h∆°n
- **Cost Reduction:** Gi·∫£m chi ph√≠ t·ªìn kho v√† thi·∫øu h√†ng

#### 6.3.2. ROI ∆∞·ªõc t√≠nh
- Gi·∫£m 10-15% chi ph√≠ t·ªìn kho
- Gi·∫£m 5-10% thi·∫øu h√†ng
- TƒÉng 2-5% doanh s·ªë nh·ªù planning t·ªët h∆°n

### 6.4. H·∫°n ch·∫ø

1. **D·ªØ li·ªáu:**
   - Ch·ªâ c√≥ d·ªØ li·ªáu t·ª´ 2010-2012 (c√≥ th·ªÉ l·ªói th·ªùi)
   - Thi·∫øu th√¥ng tin v·ªÅ promotions c·ª• th·ªÉ
   - Kh√¥ng c√≥ external factors (competitors, events)

2. **M√¥ h√¨nh:**
   - Kh√¥ng s·ª≠ d·ª•ng Deep Learning (c√≥ th·ªÉ t·ªët h∆°n cho time series)
   - Ch∆∞a th·ª≠ ensemble methods
   - Ch∆∞a optimize cho t·ª´ng store ri√™ng bi·ªát

3. **Evaluation:**
   - Ch·ªâ ƒë√°nh gi√° tr√™n test set c·ªë ƒë·ªãnh
   - Ch∆∞a c√≥ backtesting tr√™n nhi·ªÅu periods
   - Ch∆∞a c√≥ A/B testing trong production

---

## 7. PH·ª§ L·ª§C

### 7.1. C·∫•u tr√∫c d·ª± √°n

```
Machine-Learning-Assignment-251/
‚îú‚îÄ‚îÄ dataset/                          # D·ªØ li·ªáu g·ªëc
‚îÇ   ‚îú‚îÄ‚îÄ walmart-features.csv
‚îÇ   ‚îú‚îÄ‚îÄ walmart-stores.csv
‚îÇ   ‚îú‚îÄ‚îÄ walmart-train.csv
‚îÇ   ‚îî‚îÄ‚îÄ walmart-test.csv
‚îú‚îÄ‚îÄ preprocessing.py                  # EDA v√† data cleaning
‚îú‚îÄ‚îÄ ml_data_preparation.py            # Chu·∫©n b·ªã d·ªØ li·ªáu cho ML
‚îú‚îÄ‚îÄ baseline_models.py                # Baseline models
‚îú‚îÄ‚îÄ hyperparameter_tuning.py          # Hyperparameter tuning
‚îú‚îÄ‚îÄ model_evaluation_analysis.py      # ƒê√°nh gi√° v√† ph√¢n t√≠ch
‚îú‚îÄ‚îÄ train_detail.csv                  # D·ªØ li·ªáu ƒë√£ preprocess
‚îú‚îÄ‚îÄ X_train.csv, X_test.csv           # Features
‚îú‚îÄ‚îÄ y_train.csv, y_test.csv           # Targets
‚îú‚îÄ‚îÄ weights.npy                       # Weights cho WMAE
‚îú‚îÄ‚îÄ baseline_*.pkl                    # Baseline models
‚îú‚îÄ‚îÄ tuned_*.pkl                       # Tuned models
‚îú‚îÄ‚îÄ best_model.pkl                    # Best model
‚îî‚îÄ‚îÄ *.csv, *.png, *.md                # Results v√† reports
```

### 7.2. Dependencies

```python
pandas>=1.3.0
numpy>=1.21.0
matplotlib>=3.4.0
seaborn>=0.11.0
scikit-learn>=1.0.0
xgboost>=1.5.0
```

### 7.3. Th·ªùi gian th·ª±c hi·ªán

- **Data Preprocessing:** ~2 gi·ªù
- **Feature Engineering:** ~3 gi·ªù
- **Baseline Models:** ~30 ph√∫t
- **Hyperparameter Tuning:** ~10 ph√∫t
- **Evaluation & Analysis:** ~1 gi·ªù
- **T·ªïng c·ªông:** ~7 gi·ªù

### 7.4. T√†i li·ªáu tham kh·∫£o

- Walmart Sales Forecasting Competition
- Scikit-learn Documentation
- XGBoost Documentation
- Time Series Cross-Validation Best Practices

---

## 8. T√ìM T·∫ÆT EXECUTIVE

### 8.1. K·∫øt qu·∫£ ch√≠nh
- ‚úÖ **M√¥ h√¨nh t·ªët nh·∫•t:** Random Forest (Baseline)
- ‚úÖ **WMAE:** 4,602.51 (ƒë·∫°t m·ª•c ti√™u < 5,000)
- ‚úÖ **R¬≤:** 0.7523 (gi·∫£i th√≠ch 75.23% ph∆∞∆°ng sai)
- ‚úÖ **Th·ªùi gian training:** < 20 gi√¢y

### 8.2. ƒêi·ªÉm n·ªïi b·∫≠t
1. Feature engineering hi·ªáu qu·∫£ (lag, rolling, time features)
2. S·ª≠ d·ª•ng ƒë√∫ng Time Series Cross-Validation
3. Baseline model t·ªët h∆°n tuned model (b√†i h·ªçc quan tr·ªçng)
4. XGBoost Tuned c·∫£i thi·ªán 7.22% so v·ªõi baseline

### 8.3. Khuy·∫øn ngh·ªã h√†nh ƒë·ªông
1. **Tri·ªÉn khai:** S·ª≠ d·ª•ng Random Forest (Baseline) cho production
2. **Monitor:** Theo d√µi WMAE tr√™n d·ªØ li·ªáu m·ªõi
3. **C·∫£i thi·ªán:** Th·ª≠ ensemble methods v√† external data
4. **Retrain:** C·∫≠p nh·∫≠t m√¥ h√¨nh ƒë·ªãnh k·ª≥

---

**B√°o c√°o ƒë∆∞·ª£c t·∫°o b·ªüi:** Machine Learning Team  
**Ng√†y:** 2024  
**Version:** 1.0

---

*B√°o c√°o n√†y t√≥m t·∫Øt to√†n b·ªô qu√° tr√¨nh x√¢y d·ª±ng m√¥ h√¨nh d·ª± b√°o doanh s·ªë Walmart t·ª´ data preprocessing ƒë·∫øn model evaluation v√† analysis.*
