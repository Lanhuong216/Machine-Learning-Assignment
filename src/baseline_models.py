# B∆Ø·ªöC 2: BASELINE MODELS
# Ng∆∞·ªùi 2 - ML Engineer

import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import xgboost as xgb
import matplotlib.pyplot as plt
import seaborn as sns
import time
import pickle
import warnings
warnings.filterwarnings('ignore')

# Import functions t·ª´ ml_data_preparation
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from ml_data_preparation import create_evaluation_metrics, evaluate_model
from config import DATA_PROCESSED_DIR

def load_prepared_data():
    """Load d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c chu·∫©n b·ªã t·ª´ B∆Ø·ªöC 1"""
    print("=== LOAD D·ªÆ LI·ªÜU ƒê√É CHU·∫®N B·ªä ===")
    
    try:
        X_train = pd.read_csv(os.path.join(DATA_PROCESSED_DIR, 'X_train.csv'))
        X_test = pd.read_csv(os.path.join(DATA_PROCESSED_DIR, 'X_test.csv'))
        y_train = pd.read_csv(os.path.join(DATA_PROCESSED_DIR, 'y_train.csv')).iloc[:, 0]
        y_test = pd.read_csv(os.path.join(DATA_PROCESSED_DIR, 'y_test.csv')).iloc[:, 0]
        weights = np.load(os.path.join(DATA_PROCESSED_DIR, 'weights.npy'))
        
        print(f"‚úì ƒê√£ load d·ªØ li·ªáu:")
        print(f"  - X_train: {X_train.shape}")
        print(f"  - X_test: {X_test.shape}")
        print(f"  - y_train: {y_train.shape}")
        print(f"  - y_test: {y_test.shape}")
        print(f"  - weights: {weights.shape}")
        
        return X_train, X_test, y_train, y_test, weights
    
    except FileNotFoundError as e:
        print(f"Kh√¥ng t√¨m th·∫•y file: {e}")
        print("Vui l√≤ng ch·∫°y ml_data_preparation.py tr∆∞·ªõc")
        return None, None, None, None, None

def train_linear_regression(X_train, X_test, y_train, y_test, metrics=None, weights=None):
    """
    Hu·∫•n luy·ªán Linear Regression
    
    Args:
        X_train, X_test: Training v√† test features
        y_train, y_test: Training v√† test targets
        metrics (dict): Dictionary ch·ª©a c√°c h√†m ƒë√°nh gi√°
        weights (array): Weights cho WMAE
        
    Returns:
        tuple: (model, results, training_time)
    """
    print("\n=== LINEAR REGRESSION ===")
    
    start_time = time.time()
    
    # T·∫°o v√† hu·∫•n luy·ªán m√¥ h√¨nh
    model = LinearRegression()
    model.fit(X_train, y_train) # Hu·∫•n luy·ªán m√¥ h√¨nh b·∫±ng c√°ch t√¨m h·ªá s·ªë beta cho ph∆∞∆°ng tr√¨nh y = beta0 + beta1*x1 + beta2*x2 + ... + betan*xn (y l√† target, x1, x2, ..., xn l√† features)
    
    # D·ª± ƒëo√°n
    y_pred = model.predict(X_test) # D·ª± ƒëo√°n target cho test set
    
    training_time = time.time() - start_time
    
    # ƒê√°nh gi√°
    if metrics is None:
        metrics = create_evaluation_metrics()
    
    results = {}
    """
        Logic t√≠nh ƒëi·ªÉm: ƒêo·∫°n v√≤ng l·∫∑p for n√†y duy·ªát qua t·ª´ng lo·∫°i th∆∞·ªõc ƒëo (MAE, RMSE...) ƒë·ªÉ t√≠nh to√°n ƒëi·ªÉm cho m√¥ h√¨nh:
    -   N·∫øu l√† wmae: N√≥ g·ªçi h√†m t√≠nh WMAE v√† truy·ªÅn th√™m tham s·ªë weights (tr·ªçng s·ªë ng√†y l·ªÖ) v√†o.
    -   N·∫øu l√† th∆∞·ªõc ƒëo th∆∞·ªùng (MAE, RMSE): N√≥ ch·ªâ c·∫ßn so s√°nh ƒë√°p √°n th·ª±c (y_test) v√† d·ª± ƒëo√°n (y_pred).
    """
    for metric_name, metric_func in metrics.items():
        if metric_name == 'wmae' and weights is not None:
            results[metric_name] = metric_func(y_test, y_pred, weights)
        else:
            results[metric_name] = metric_func(y_test, y_pred)
    
    results['training_time'] = training_time
    
    print(f"Training time: {training_time:.2f} seconds")
    print(f"MAE: {results['mae']:.2f}")
    print(f"RMSE: {results['rmse']:.2f}")
    print(f"R¬≤: {results['r2']:.4f}")
    print(f"WMAE: {results['wmae']:.2f}")
    
    return model, results, training_time

def train_random_forest(X_train, X_test, y_train, y_test, metrics=None, weights=None,
                       n_estimators=100, max_depth=None, random_state=42):
    """
    Hu·∫•n luy·ªán Random Forest
    
    Args:
        X_train, X_test: Training v√† test features
        y_train, y_test: Training v√† test targets
        metrics (dict): Dictionary ch·ª©a c√°c h√†m ƒë√°nh gi√°
        weights (array): Weights cho WMAE
        n_estimators (int): S·ªë c√¢y
        max_depth (int): ƒê·ªô s√¢u t·ªëi ƒëa
        random_state (int): Random seed
        
    Returns:
        tuple: (model, results, training_time)
    """
    print("\n=== RANDOM FOREST ===")
    
    start_time = time.time()
    
    # T·∫°o v√† hu·∫•n luy·ªán m√¥ h√¨nh
    model = RandomForestRegressor(
        n_estimators=n_estimators,
        max_depth=max_depth,
        random_state=random_state,
        n_jobs=-1, #S·ª≠ d·ª•ng s·ªë l∆∞·ª£ng CPU (-1 = maximum)
        verbose=0
    )
    model.fit(X_train, y_train) # Hu·∫•n luy·ªán m√¥ h√¨nh b·∫±ng c√°ch t√¨m c√°c c√¢y quy·∫øt ƒë·ªãnh cho Random Forest
    
    # D·ª± ƒëo√°n
    y_pred = model.predict(X_test)
    
    training_time = time.time() - start_time
    
    # ƒê√°nh gi√°
    if metrics is None:
        metrics = create_evaluation_metrics()
    
    results = {}
    for metric_name, metric_func in metrics.items():
        if metric_name == 'wmae' and weights is not None:
            results[metric_name] = metric_func(y_test, y_pred, weights)
        else:
            results[metric_name] = metric_func(y_test, y_pred)
    
    results['training_time'] = training_time
    
    print(f"Training time: {training_time:.2f} seconds")
    print(f"MAE: {results['mae']:.2f}")
    print(f"RMSE: {results['rmse']:.2f}")
    print(f"R¬≤: {results['r2']:.4f}")
    print(f"WMAE: {results['wmae']:.2f}")
    
    return model, results, training_time

def train_xgboost(X_train, X_test, y_train, y_test, metrics=None, weights=None,
                 n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42):
    """
    Hu·∫•n luy·ªán XGBoost
    
    Args:
        X_train, X_test: Training v√† test features
        y_train, y_test: Training v√† test targets
        metrics (dict): Dictionary ch·ª©a c√°c h√†m ƒë√°nh gi√°
        weights (array): Weights cho WMAE
        n_estimators (int): S·ªë c√¢y
        learning_rate (float): Learning rate (t·ªëc ƒë·ªô h·ªçc, s·ªë c√†ng nh·ªè th√¨ m√¥ h√¨nh h·ªçc c√†ng ch·∫≠m nh∆∞ng ch√≠nh x√°c h∆°n)
        max_depth (int): ƒê·ªô s√¢u t·ªëi ƒëa (s·ªë l∆∞·ª£ng c√¢y con t·ªëi ƒëa m√† m·ªói c√¢y quy·∫øt ƒë·ªãnh c√≥ th·ªÉ c√≥, m·∫∑c ƒë·ªãnh l√† 6 v√¨ XGBoost l√† m√¥ h√¨nh c√≥ ƒë·ªô s√¢u t·ªëi ƒëa l√† 6)
        random_state (int): Random seed (seed l√† m·ªôt s·ªë nguy√™n ƒë·ªÉ kh·ªüi t·∫°o m·ªôt gi√° tr·ªã ng·∫´u nhi√™n, ƒë·ªÉ ƒë·∫£m b·∫£o r·∫±ng k·∫øt qu·∫£ c·ªßa m√¥ h√¨nh l√† reproducible)
        
    Returns:
        tuple: (model, results, training_time)
    """
    print("\n=== XGBOOST ===")
    
    start_time = time.time()
    
    # T·∫°o v√† hu·∫•n luy·ªán m√¥ h√¨nh
    model = xgb.XGBRegressor(
        n_estimators=n_estimators,
        learning_rate=learning_rate,
        max_depth=max_depth,
        random_state=random_state,
        n_jobs=-1, #S·ª≠ d·ª•ng s·ªë l∆∞·ª£ng CPU (-1 = maximum)
        verbosity=0
    )
    model.fit(X_train, y_train) # Hu·∫•n luy·ªán m√¥ h√¨nh b·∫±ng XGBoost
    
    # D·ª± ƒëo√°n
    y_pred = model.predict(X_test) # D·ª± ƒëo√°n target cho test set
    
    training_time = time.time() - start_time
    
    # ƒê√°nh gi√°
    if metrics is None:
        metrics = create_evaluation_metrics()
    
    results = {}
    for metric_name, metric_func in metrics.items():
        if metric_name == 'wmae' and weights is not None:
            results[metric_name] = metric_func(y_test, y_pred, weights)
        else:
            results[metric_name] = metric_func(y_test, y_pred)
    
    results['training_time'] = training_time
    
    print(f"Training time: {training_time:.2f} seconds")
    print(f"MAE: {results['mae']:.2f}")
    print(f"RMSE: {results['rmse']:.2f}")
    print(f"R¬≤: {results['r2']:.4f}")
    print(f"WMAE: {results['wmae']:.2f}")
    
    return model, results, training_time

def compare_baseline_models(models_results):
    """
    So s√°nh c√°c m√¥ h√¨nh c∆° s·ªü
    
    Args:
        models_results (dict): Dictionary ch·ª©a k·∫øt qu·∫£ c√°c m√¥ h√¨nh
        
    Returns:
        pd.DataFrame: B·∫£ng so s√°nh
    """
    print("\n=== SO S√ÅNH C√ÅC M√î H√åNH C∆† S·ªû ===")
    
    # T·∫°o b·∫£ng so s√°nh
    comparison_data = []
    for model_name, results in models_results.items():
        comparison_data.append({
            'Model': model_name,
            'MAE': results['mae'],
            'MSE': results['mse'],
            'RMSE': results['rmse'],
            'R¬≤': results['r2'],
            'WMAE': results['wmae'],
            'Training Time (s)': results['training_time']
        })
    
    comparison_df = pd.DataFrame(comparison_data)
    comparison_df = comparison_df.round(4)
    
    # S·∫Øp x·∫øp theo WMAE (metric ch√≠nh)
    comparison_df = comparison_df.sort_values('WMAE')
    
    print("\n" + "="*80)
    print(comparison_df.to_string(index=False))
    print("="*80)
    
    # T√¨m m√¥ h√¨nh t·ªët nh·∫•t
    best_model_wmae = comparison_df.iloc[0]['Model']
    best_model_mae = comparison_df.loc[comparison_df['MAE'].idxmin(), 'Model']
    best_model_r2 = comparison_df.loc[comparison_df['R¬≤'].idxmax(), 'Model']
    
    print(f"\nüèÜ M√¥ h√¨nh t·ªët nh·∫•t theo WMAE: {best_model_wmae}")
    print(f"üèÜ M√¥ h√¨nh t·ªët nh·∫•t theo MAE: {best_model_mae}")
    print(f"üèÜ M√¥ h√¨nh t·ªët nh·∫•t theo R¬≤: {best_model_r2}")
    
    return comparison_df

def visualize_baseline_results(models_results):
    """
    Tr·ª±c quan h√≥a k·∫øt qu·∫£ c√°c m√¥ h√¨nh c∆° s·ªü
    
    Args:
        models_results (dict): Dictionary ch·ª©a k·∫øt qu·∫£ c√°c m√¥ h√¨nh
    """
    print("\n=== TR·ª∞C QUAN H√ìA K·∫æT QU·∫¢ ===")
    
    n_models = len(models_results)
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # 1. So s√°nh MAE
    axes[0, 0].bar(models_results.keys(), [results['mae'] for results in models_results.values()])
    axes[0, 0].set_title('MAE Comparison', fontsize=14, fontweight='bold')
    axes[0, 0].set_ylabel('MAE')
    axes[0, 0].tick_params(axis='x', rotation=45)
    axes[0, 0].grid(True, alpha=0.3, axis='y')
    
    # 2. So s√°nh RMSE
    axes[0, 1].bar(models_results.keys(), [results['rmse'] for results in models_results.values()])
    axes[0, 1].set_title('RMSE Comparison', fontsize=14, fontweight='bold')
    axes[0, 1].set_ylabel('RMSE')
    axes[0, 1].tick_params(axis='x', rotation=45)
    axes[0, 1].grid(True, alpha=0.3, axis='y')
    
    # 3. So s√°nh R¬≤
    axes[1, 0].bar(models_results.keys(), [results['r2'] for results in models_results.values()])
    axes[1, 0].set_title('R¬≤ Comparison', fontsize=14, fontweight='bold')
    axes[1, 0].set_ylabel('R¬≤')
    axes[1, 0].tick_params(axis='x', rotation=45)
    axes[1, 0].grid(True, alpha=0.3, axis='y')
    
    # 4. So s√°nh WMAE (metric ch√≠nh)
    axes[1, 1].bar(models_results.keys(), [results['wmae'] for results in models_results.values()], color='coral')
    axes[1, 1].set_title('WMAE Comparison (Primary Metric)', fontsize=14, fontweight='bold')
    axes[1, 1].set_ylabel('WMAE')
    axes[1, 1].tick_params(axis='x', rotation=45)
    axes[1, 1].grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    from config import OUTPUT_VISUALIZATIONS_DIR
    output_path = os.path.join(OUTPUT_VISUALIZATIONS_DIR, 'baseline_models_comparison.png')
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"‚úì ƒê√£ l∆∞u bi·ªÉu ƒë·ªì: {output_path}")
    plt.show()

def save_baseline_models(models_dict):
    """
    L∆∞u c√°c m√¥ h√¨nh c∆° s·ªü
    
    Args:
        models_dict (dict): Dictionary ch·ª©a c√°c m√¥ h√¨nh
    """
    print("\n=== L∆ØU C√ÅC M√î H√åNH C∆† S·ªû ===")
    
    from config import MODELS_DIR
    for model_name, model in models_dict.items():
        filename = f"baseline_{model_name.lower().replace(' ', '_')}_model.pkl"
        filepath = os.path.join(MODELS_DIR, filename)
        
        with open(filepath, 'wb') as f:
            pickle.dump(model, f)
        
        print(f"‚úì ƒê√£ l∆∞u: {filepath}")

def main():
    """H√†m ch√≠nh ƒë·ªÉ ch·∫°y B∆Ø·ªöC 2"""
    print("="*80)
    print("B∆Ø·ªöC 2: BASELINE MODELS")
    print("="*80)
    
    # Load d·ªØ li·ªáu ƒë√£ chu·∫©n b·ªã
    X_train, X_test, y_train, y_test, weights = load_prepared_data()
    
    if X_train is None:
        print("\n‚ùå Kh√¥ng th·ªÉ ti·∫øp t·ª•c do thi·∫øu d·ªØ li·ªáu")
        return None
    
    # T·∫°o evaluation metrics
    metrics = create_evaluation_metrics()
    
    # Hu·∫•n luy·ªán c√°c m√¥ h√¨nh c∆° s·ªü
    models = {}
    models_results = {}
    
    # 1. Linear Regression
    lr_model, lr_results, lr_time = train_linear_regression(
        X_train, X_test, y_train, y_test, metrics, weights
    )
    models['Linear Regression'] = lr_model
    models_results['Linear Regression'] = lr_results
    
    # 2. Random Forest
    rf_model, rf_results, rf_time = train_random_forest(
        X_train, X_test, y_train, y_test, metrics, weights
    )
    models['Random Forest'] = rf_model
    models_results['Random Forest'] = rf_results
    
    # 3. XGBoost
    xgb_model, xgb_results, xgb_time = train_xgboost(
        X_train, X_test, y_train, y_test, metrics, weights
    )
    models['XGBoost'] = xgb_model
    models_results['XGBoost'] = xgb_results
    
    # So s√°nh c√°c m√¥ h√¨nh
    comparison_df = compare_baseline_models(models_results)
    
    # Tr·ª±c quan h√≥a
    visualize_baseline_results(models_results)
    
    # L∆∞u m√¥ h√¨nh
    save_baseline_models(models)
    
    # L∆∞u k·∫øt qu·∫£
    from config import OUTPUT_REPORTS_DIR
    output_path = os.path.join(OUTPUT_REPORTS_DIR, 'baseline_models_comparison.csv')
    comparison_df.to_csv(output_path, index=False)
    print(f"\n‚úì ƒê√£ l∆∞u k·∫øt qu·∫£: {output_path}")
    
    print("\n" + "="*80)
    print("‚úì HO√ÄN TH√ÄNH B∆Ø·ªöC 2: BASELINE MODELS")
    print("="*80)
    print("\nC√°c file ƒë√£ ƒë∆∞·ª£c t·∫°o:")
    print("  - baseline_linear_regression_model.pkl")
    print("  - baseline_random_forest_model.pkl")
    print("  - baseline_xgboost_model.pkl")
    print("  - baseline_models_comparison.csv")
    print("  - baseline_models_comparison.png")
    print("\nB√¢y gi·ªù c√≥ th·ªÉ ti·∫øp t·ª•c B∆Ø·ªöC 3 (Advanced Models & Hyperparameter Tuning)")
    
    return {
        'models': models,
        'results': models_results,
        'comparison': comparison_df
    }

if __name__ == "__main__":
    results = main()
