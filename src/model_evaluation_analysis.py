# B∆Ø·ªöC 4: MODEL EVALUATION & ANALYSIS
# So s√°nh models t·ª´ k-fold validation results

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# Import functions
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from config import OUTPUT_DIR, OUTPUT_VISUALIZATIONS_DIR, OUTPUT_REPORTS_DIR, DATA_PROCESSED_DIR

def load_kfold_comparison_results():
    """
    Load k·∫øt qu·∫£ k-fold validation t·ª´ CSV files
    
    Returns:
        tuple: (untuned_results_df, tuned_results_df) ho·∫∑c (None, None) n·∫øu l·ªói
    """
    print("=== LOAD K·∫æT QU·∫¢ K-FOLD VALIDATION ===")
    
    untuned_path = os.path.join(OUTPUT_DIR, 'kfold_validation_comparison.csv')
    tuned_path = os.path.join(OUTPUT_DIR, 'best_params_kfold_comparison.csv')
    
    untuned_df = None
    tuned_df = None
    
    # Load untuned results
    if os.path.exists(untuned_path):
        try:
            untuned_df = pd.read_csv(untuned_path)
            print(f"‚úì ƒê√£ load untuned results: {len(untuned_df)} models")
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói khi load {untuned_path}: {e}")
    else:
        print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y: {untuned_path}")
    
    # Load tuned results
    if os.path.exists(tuned_path):
        try:
            tuned_df = pd.read_csv(tuned_path)
            print(f"‚úì ƒê√£ load tuned results: {len(tuned_df)} models")
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói khi load {tuned_path}: {e}")
    else:
        print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y: {tuned_path}")
    
    if untuned_df is None and tuned_df is None:
        print("‚ùå Kh√¥ng c√≥ k·∫øt qu·∫£ n√†o ƒë·ªÉ so s√°nh")
        return None, None
    
    return untuned_df, tuned_df

def create_comparison_from_csv(untuned_df, tuned_df):
    """
    T·∫°o b·∫£ng so s√°nh t·ª´ k·∫øt qu·∫£ CSV
    
    Args:
        untuned_df: DataFrame ch·ª©a k·∫øt qu·∫£ models ch∆∞a tuning
        tuned_df: DataFrame ch·ª©a k·∫øt qu·∫£ models ƒë√£ tuning
        
    Returns:
        pd.DataFrame: B·∫£ng so s√°nh
    """
    print("\n=== T·∫†O B·∫¢NG SO S√ÅNH ===")
    
    comparison_data = []
    
    # Th√™m untuned models
    if untuned_df is not None:
        for _, row in untuned_df.iterrows():
            comparison_data.append({
                'Model': f"{row['Model']} (Untuned)",
                'Mean_WMAE': row['Mean_WMAE'],
                'Std_WMAE': row['Std_WMAE'],
                'Mean_MAE': row['Mean_MAE'],
                'Std_MAE': row['Std_MAE'],
                'Mean_RMSE': row['Mean_RMSE'],
                'Std_RMSE': row['Std_RMSE'],
                'Mean_R2': row['Mean_R2'],
                'Std_R2': row['Std_R2'],
                'Mean_Train_Time': row['Mean_Train_Time']
            })
    
    # Th√™m tuned models
    if tuned_df is not None:
        for _, row in tuned_df.iterrows():
            comparison_data.append({
                'Model': f"{row['Model']} (Tuned)",
                'Mean_WMAE': row['Mean_WMAE'],
                'Std_WMAE': row['Std_WMAE'],
                'Mean_MAE': row['Mean_MAE'],
                'Std_MAE': row['Std_MAE'],
                'Mean_RMSE': row['Mean_RMSE'],
                'Std_RMSE': row['Std_RMSE'],
                'Mean_R2': row['Mean_R2'],
                'Std_R2': row['Std_R2'],
                'Mean_Train_Time': row['Mean_Train_Time']
            })
    
    if len(comparison_data) == 0:
        print("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ so s√°nh")
        return None
    
    comparison_df = pd.DataFrame(comparison_data)
    comparison_df = comparison_df.sort_values('Mean_WMAE')
    
    return comparison_df

def create_final_comparison(comparison_df):
    """T·∫°o b·∫£ng so s√°nh cu·ªëi c√πng t·ª´ comparison_df"""
    print("\n=== B·∫¢NG SO S√ÅNH CU·ªêI C√ôNG ===")
    
    if comparison_df is None or len(comparison_df) == 0:
        print("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ so s√°nh")
        return None, None
    
    # Format ƒë·ªÉ hi·ªÉn th·ªã ƒë·∫πp
    display_df = comparison_df.copy()
    display_df = display_df.round(4)
    
    print("\n" + "="*100)
    print(display_df.to_string(index=False))
    print("="*100)
    
    # T√¨m m√¥ h√¨nh t·ªët nh·∫•t
    best_model = comparison_df.iloc[0]
    print(f"\nüèÜ M√î H√åNH T·ªêT NH·∫§T: {best_model['Model']}")
    print(f"   Mean WMAE: {best_model['Mean_WMAE']:.2f} ¬± {best_model['Std_WMAE']:.2f}")
    print(f"   Mean MAE: {best_model['Mean_MAE']:.2f} ¬± {best_model['Std_MAE']:.2f}")
    print(f"   Mean R¬≤: {best_model['Mean_R2']:.4f} ¬± {best_model['Std_R2']:.4f}")
    
    return comparison_df, best_model

def visualize_comparison(comparison_df):
    """Tr·ª±c quan h√≥a k·∫øt qu·∫£ so s√°nh"""
    print("\n=== TR·ª∞C QUAN H√ìA K·∫æT QU·∫¢ SO S√ÅNH ===")
    
    if comparison_df is None or len(comparison_df) == 0:
        print("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ visualize")
        return
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    model_names = comparison_df['Model'].tolist()
    
    # 1. WMAE Comparison
    mean_wmae = comparison_df['Mean_WMAE'].values
    std_wmae = comparison_df['Std_WMAE'].values
    axes[0, 0].barh(model_names, mean_wmae, xerr=std_wmae, capsize=5, color='steelblue')
    axes[0, 0].set_xlabel('WMAE', fontsize=12)
    axes[0, 0].set_title('WMAE Comparison (K-Fold CV)', fontsize=14, fontweight='bold')
    axes[0, 0].grid(True, alpha=0.3, axis='x')
    axes[0, 0].invert_yaxis()
    
    # 2. R¬≤ Comparison
    mean_r2 = comparison_df['Mean_R2'].values
    std_r2 = comparison_df['Std_R2'].values
    axes[0, 1].barh(model_names, mean_r2, xerr=std_r2, capsize=5, color='coral')
    axes[0, 1].set_xlabel('R¬≤', fontsize=12)
    axes[0, 1].set_title('R¬≤ Comparison (K-Fold CV)', fontsize=14, fontweight='bold')
    axes[0, 1].grid(True, alpha=0.3, axis='x')
    axes[0, 1].invert_yaxis()
    
    # 3. MAE Comparison
    mean_mae = comparison_df['Mean_MAE'].values
    std_mae = comparison_df['Std_MAE'].values
    axes[1, 0].barh(model_names, mean_mae, xerr=std_mae, capsize=5, color='lightgreen')
    axes[1, 0].set_xlabel('MAE', fontsize=12)
    axes[1, 0].set_title('MAE Comparison (K-Fold CV)', fontsize=14, fontweight='bold')
    axes[1, 0].grid(True, alpha=0.3, axis='x')
    axes[1, 0].invert_yaxis()
    
    # 4. Train Time Comparison
    mean_time = comparison_df['Mean_Train_Time'].values
    axes[1, 1].barh(model_names, mean_time, color='gold')
    axes[1, 1].set_xlabel('Train Time (seconds)', fontsize=12)
    axes[1, 1].set_title('Training Time Comparison', fontsize=14, fontweight='bold')
    axes[1, 1].grid(True, alpha=0.3, axis='x')
    axes[1, 1].invert_yaxis()
    
    plt.tight_layout()
    output_path = os.path.join(OUTPUT_VISUALIZATIONS_DIR, 'model_comparison_visualization.png')
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"‚úì ƒê√£ l∆∞u bi·ªÉu ƒë·ªì: {output_path}")

def create_final_report(comparison_df, best_model):
    """T·∫°o b√°o c√°o cu·ªëi c√πng"""
    print("\n=== T·∫†O B√ÅO C√ÅO CU·ªêI C√ôNG ===")
    
    from datetime import datetime
    
    if comparison_df is None or best_model is None:
        print("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ t·∫°o b√°o c√°o")
        return None
    
    # Format comparison table cho report
    display_df = comparison_df.copy()
    display_df = display_df.round(4)
    
    # B·∫Øt ƒë·∫ßu t·∫°o report
    report_lines = [
        "# B√ÅO C√ÅO CU·ªêI C√ôNG - WALMART SALES FORECASTING",
        "",
        f"**Ng√†y t·∫°o:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        "",
        "## T·ªîNG QUAN",
        "",
        "D·ª± √°n d·ª± b√°o doanh s·ªë Walmart s·ª≠ d·ª•ng Machine Learning truy·ªÅn th·ªëng (KH√îNG s·ª≠ d·ª•ng Deep Learning).",
        "S·ª≠ d·ª•ng K-Fold Cross-Validation (K=5) ƒë·ªÉ ƒë√°nh gi√° v√† so s√°nh c√°c m√¥ h√¨nh m·ªôt c√°ch robust.",
        "",
        "## WORKFLOW",
        "",
        "Pipeline ƒë∆∞·ª£c th·ª±c hi·ªán theo c√°c b∆∞·ªõc sau:",
        "",
        "1. **Preprocessing** (`preprocessing.py`):",
        "   - Load v√† merge d·ªØ li·ªáu t·ª´ c√°c file raw",
        "   - Feature engineering (Week, Month, Year, Day)",
        "   - Ch·ªçn features v√† l∆∞u `train_detail.csv`, `test_detail.csv`, `feature_chosen.csv`",
        "",
        "2. **K-Fold Validation - Untuned Models** (`k_fold_validation.py`):",
        "   - Ch·∫°y K-Fold Cross-Validation cho Random Forest v√† XGBoost v·ªõi default parameters",
        "   - L∆∞u k·∫øt qu·∫£ v√†o `kfold_validation_comparison.csv`",
        "",
        "3. **Hyperparameter Tuning** (`hyperparameter_tuning.py`):",
        "   - S·ª≠ d·ª•ng RandomizedSearchCV v·ªõi TimeSeriesSplit ƒë·ªÉ t√¨m best parameters",
        "   - L∆∞u best parameters v√†o `tuned_models_best_params.csv`",
        "",
        "4. **Train With Best Params** (`train_with_best_params.py`):",
        "   - Load best parameters t·ª´ CSV",
        "   - Ch·∫°y K-Fold Cross-Validation v·ªõi best parameters",
        "   - Train model t·ªët nh·∫•t v√† t·∫°o submission file",
        "   - L∆∞u k·∫øt qu·∫£ v√†o `best_params_kfold_comparison.csv`",
        "",
        "5. **Model Evaluation & Analysis** (`model_evaluation_analysis.py`):",
        "   - So s√°nh k·∫øt qu·∫£ untuned vs tuned models",
        "   - T·∫°o visualization v√† final report",
        "",
        "## K·∫æT QU·∫¢ C√ÅC M√î H√åNH (K-FOLD CROSS-VALIDATION)",
        "",
        "```",
        display_df.to_string(index=False),
        "```",
        "",
        "## M√î H√åNH T·ªêT NH·∫§T",
        "",
        f"**Model:** {best_model['Model']}",
        f"- **Mean WMAE:** {best_model['Mean_WMAE']:.2f} ¬± {best_model['Std_WMAE']:.2f} (Metric ch√≠nh c·ªßa cu·ªôc thi)",
        f"- **Mean MAE:** {best_model['Mean_MAE']:.2f} ¬± {best_model['Std_MAE']:.2f}",
        f"- **Mean RMSE:** {best_model['Mean_RMSE']:.2f} ¬± {best_model['Std_RMSE']:.2f}",
        f"- **Mean R¬≤:** {best_model['Mean_R2']:.4f} ¬± {best_model['Std_R2']:.4f}",
        f"- **Mean Train Time:** {best_model['Mean_Train_Time']:.2f}s",
        "",
        "## PH√ÇN T√çCH K·∫æT QU·∫¢",
        "",
        "### 1. Models Ch∆∞a Tuning (Untuned)"
    ]
    
    # Th√™m th√¥ng tin untuned models
    untuned_models = comparison_df[comparison_df['Model'].str.contains('Untuned', case=False, na=False)]
    if not untuned_models.empty:
        for _, row in untuned_models.iterrows():
            model_name = row['Model'].replace(' (Untuned)', '')
            report_lines.extend([
                f"- **{model_name} (Untuned):** Mean WMAE = {row['Mean_WMAE']:.2f} ¬± {row['Std_WMAE']:.2f}",
                f"  - Mean R¬≤: {row['Mean_R2']:.4f} ¬± {row['Std_R2']:.4f}",
                ""
            ])
    
    report_lines.extend([
        "### 2. Models ƒê√£ Tuning (Tuned)"
    ])
    
    # Th√™m th√¥ng tin tuned models
    tuned_models = comparison_df[comparison_df['Model'].str.contains('Tuned', case=False, na=False)]
    if not tuned_models.empty:
        for _, row in tuned_models.iterrows():
            model_name = row['Model'].replace(' (Tuned)', '')
            # T√¨m untuned t∆∞∆°ng ·ª©ng ƒë·ªÉ so s√°nh
            untuned_row = untuned_models[untuned_models['Model'].str.contains(model_name, case=False, na=False)]
            if not untuned_row.empty:
                improvement = ((untuned_row['Mean_WMAE'].iloc[0] - row['Mean_WMAE']) / untuned_row['Mean_WMAE'].iloc[0] * 100)
                report_lines.extend([
                    f"- **{model_name} (Tuned):** Mean WMAE = {row['Mean_WMAE']:.2f} ¬± {row['Std_WMAE']:.2f}",
                    f"  - Mean R¬≤: {row['Mean_R2']:.4f} ¬± {row['Std_R2']:.4f}",
                    f"  - C·∫£i thi·ªán: {improvement:.2f}% so v·ªõi Untuned",
                    ""
                ])
            else:
                report_lines.extend([
                    f"- **{model_name} (Tuned):** Mean WMAE = {row['Mean_WMAE']:.2f} ¬± {row['Std_WMAE']:.2f}",
                    f"  - Mean R¬≤: {row['Mean_R2']:.4f} ¬± {row['Std_R2']:.4f}",
                    ""
                ])
    
    report_lines.extend([
        "## KHUY·∫æN NGH·ªä",
        "",
        "### Cho Production:",
        f"1. **S·ª≠ d·ª•ng m√¥ h√¨nh:** {best_model['Model']}",
        f"2. **Mean WMAE ƒë·∫°t ƒë∆∞·ª£c:** {best_model['Mean_WMAE']:.2f} ¬± {best_model['Std_WMAE']:.2f}",
        "3. **Monitor performance:** Theo d√µi WMAE tr√™n d·ªØ li·ªáu m·ªõi",
        "4. **Retrain ƒë·ªãnh k·ª≥:** C·∫≠p nh·∫≠t m√¥ h√¨nh v·ªõi d·ªØ li·ªáu m·ªõi",
        "",
        "## K·∫æT LU·∫¨N",
        "",
        f"D·ª± √°n ƒë√£ th√†nh c√¥ng trong vi·ªác x√¢y d·ª±ng c√°c m√¥ h√¨nh d·ª± b√°o doanh s·ªë Walmart v·ªõi hi·ªáu su·∫•t cao.",
        f"M√¥ h√¨nh t·ªët nh·∫•t ƒë·∫°t ƒë∆∞·ª£c Mean WMAE = {best_model['Mean_WMAE']:.2f} ¬± {best_model['Std_WMAE']:.2f},",
        "cho th·∫•y kh·∫£ nƒÉng d·ª± b√°o ch√≠nh x√°c v√† ƒë√°ng tin c·∫≠y.",
        "",
        "Vi·ªác s·ª≠ d·ª•ng K-Fold Cross-Validation v√† t·∫≠p trung v√†o WMAE metric ƒë·∫£m b·∫£o",
        "m√¥ h√¨nh s·∫Ω ho·∫°t ƒë·ªông t·ªët trong th·ª±c t·∫ø."
    ])
    
    report = "\n".join(report_lines)
    
    output_path = os.path.join(OUTPUT_REPORTS_DIR, 'final_report.md')
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"‚úì ƒê√£ l∆∞u b√°o c√°o: {output_path}")
    return report

def main():
    """H√†m ch√≠nh ƒë·ªÉ ch·∫°y B∆Ø·ªöC 4"""
    print("="*80)
    print("B∆Ø·ªöC 4: MODEL EVALUATION & ANALYSIS")
    print("="*80)
    
    # Load k·∫øt qu·∫£ k-fold validation t·ª´ CSV
    untuned_df, tuned_df = load_kfold_comparison_results()
    if untuned_df is None and tuned_df is None:
        print("\n‚ùå Kh√¥ng c√≥ k·∫øt qu·∫£ ƒë·ªÉ so s√°nh")
        print("   Vui l√≤ng ch·∫°y k_fold_validation.py v√† train_with_best_params.py tr∆∞·ªõc")
        return None
    
    # T·∫°o b·∫£ng so s√°nh
    comparison_df = create_comparison_from_csv(untuned_df, tuned_df)
    if comparison_df is None:
        return None
    
    # T·∫°o b·∫£ng so s√°nh cu·ªëi c√πng
    comparison_df_final, best_model = create_final_comparison(comparison_df)
    
    # Tr·ª±c quan h√≥a
    visualize_comparison(comparison_df)
    
    # T·∫°o b√°o c√°o cu·ªëi c√πng
    final_report = create_final_report(comparison_df, best_model)
    
    # L∆∞u k·∫øt qu·∫£
    comparison_path = os.path.join(OUTPUT_REPORTS_DIR, 'final_model_comparison.csv')
    comparison_df.to_csv(comparison_path, index=False)
    print(f"\n‚úì ƒê√£ l∆∞u: {comparison_path}")
    
    print("\n" + "="*80)
    print("‚úì HO√ÄN TH√ÄNH B∆Ø·ªöC 4: MODEL EVALUATION & ANALYSIS")
    print("="*80)
    print("\nC√°c file ƒë√£ ƒë∆∞·ª£c t·∫°o:")
    print("  - final_model_comparison.csv")
    print("  - final_report.md")
    print("  - model_comparison_visualization.png")
    print("\nüéâ HO√ÄN TH√ÄNH D·ª∞ √ÅN!")
    
    return {
        'comparison': comparison_df,
        'best_model': best_model,
        'report': final_report
    }

if __name__ == "__main__":
    results = main()
