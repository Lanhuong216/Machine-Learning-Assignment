# B∆Ø·ªöC 4: MODEL EVALUATION & ANALYSIS
# Ng∆∞·ªùi 2 - ML Engineer

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import warnings
warnings.filterwarnings('ignore')

# Import functions t·ª´ ml_data_preparation
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from ml_data_preparation import create_evaluation_metrics
from config import MODELS_DIR, DATA_PROCESSED_DIR, OUTPUT_VISUALIZATIONS_DIR, OUTPUT_REPORTS_DIR

def load_all_models():
    """Load t·∫•t c·∫£ c√°c m√¥ h√¨nh ƒë√£ train"""
    print("=== LOAD T·∫§T C·∫¢ C√ÅC M√î H√åNH ===")
    
    models = {}
    model_files = {
        'Linear Regression (Baseline)': 'baseline_linear_regression_model.pkl',
        'Random Forest (Baseline)': 'baseline_random_forest_model.pkl',
        'XGBoost (Baseline)': 'baseline_xgboost_model.pkl',
        'Random Forest (Tuned)': 'tuned_random_forest_model.pkl',
        'XGBoost (Tuned)': 'tuned_xgboost_model.pkl'
    }
    
    for model_name, model_file in model_files.items():
        try:
            filepath = os.path.join(MODELS_DIR, model_file)
            with open(filepath, 'rb') as f:
                model = pickle.load(f)
                models[model_name] = model
                print(f"‚úì ƒê√£ load: {model_name}")
        except FileNotFoundError:
            print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y: {filepath}")
    
    print(f"\n‚úì T·ªïng c·ªông load ƒë∆∞·ª£c {len(models)} m√¥ h√¨nh")
    return models

def load_prepared_data():
    """Load d·ªØ li·ªáu test"""
    print("\n=== LOAD D·ªÆ LI·ªÜU TEST ===")
    
    try:
        X_test = pd.read_csv(os.path.join(DATA_PROCESSED_DIR, 'X_test.csv'))
        y_test = pd.read_csv(os.path.join(DATA_PROCESSED_DIR, 'y_test.csv')).iloc[:, 0]
        weights = np.load(os.path.join(DATA_PROCESSED_DIR, 'weights.npy'))
        feature_names = pd.read_csv(os.path.join(DATA_PROCESSED_DIR, 'feature_names.csv')).iloc[:, 0].tolist()
        
        print(f"‚úì ƒê√£ load:")
        print(f"  - X_test: {X_test.shape}")
        print(f"  - y_test: {y_test.shape}")
        print(f"  - weights: {weights.shape}")
        print(f"  - features: {len(feature_names)}")
        
        return X_test, y_test, weights, feature_names
    
    except FileNotFoundError as e:
        print(f"Kh√¥ng t√¨m th·∫•y file: {e}")
        return None, None, None, None

def evaluate_all_models(models, X_test, y_test, weights, metrics=None):
    """ƒê√°nh gi√° t·∫•t c·∫£ c√°c m√¥ h√¨nh"""
    print("\n=== ƒê√ÅNH GI√Å T·∫§T C·∫¢ C√ÅC M√î H√åNH ===")
    
    if metrics is None:
        metrics = create_evaluation_metrics()
    
    results = {}
    
    for model_name, model in models.items():
        print(f"\nƒê√°nh gi√° {model_name}...")
        
        # D·ª± ƒëo√°n
        y_pred = model.predict(X_test)
        
        # T√≠nh metrics
        model_results = {}
        for metric_name, metric_func in metrics.items():
            if metric_name == 'wmae' and weights is not None:
                model_results[metric_name] = metric_func(y_test, y_pred, weights)
            else:
                model_results[metric_name] = metric_func(y_test, y_pred)
        
        results[model_name] = model_results
        
        print(f"  MAE: {model_results['mae']:.2f}")
        print(f"  RMSE: {model_results['rmse']:.2f}")
        print(f"  R¬≤: {model_results['r2']:.4f}")
        print(f"  WMAE: {model_results['wmae']:.2f}")
    
    return results

def create_final_comparison(results):
    """T·∫°o b·∫£ng so s√°nh cu·ªëi c√πng"""
    print("\n=== B·∫¢NG SO S√ÅNH CU·ªêI C√ôNG ===")
    
    comparison_data = []
    for model_name, model_results in results.items():
        comparison_data.append({
            'Model': model_name,
            'MAE': model_results['mae'],
            'RMSE': model_results['rmse'],
            'R¬≤': model_results['r2'],
            'WMAE': model_results['wmae'],
            'MAPE (%)': model_results['mape']
        })
    
    comparison_df = pd.DataFrame(comparison_data)
    comparison_df = comparison_df.round(4)
    comparison_df = comparison_df.sort_values('WMAE')
    
    print("\n" + "="*100)
    print(comparison_df.to_string(index=False))
    print("="*100)
    
    # T√¨m m√¥ h√¨nh t·ªët nh·∫•t
    best_model = comparison_df.iloc[0]
    print(f"\nüèÜ M√î H√åNH T·ªêT NH·∫§T: {best_model['Model']}")
    print(f"   WMAE: {best_model['WMAE']:.2f}")
    print(f"   MAE: {best_model['MAE']:.2f}")
    print(f"   R¬≤: {best_model['R¬≤']:.4f}")
    
    return comparison_df, best_model

def analyze_feature_importance(models, feature_names, top_n=15):
    """Ph√¢n t√≠ch feature importance"""
    print(f"\n=== PH√ÇN T√çCH FEATURE IMPORTANCE (Top {top_n}) ===")
    
    # Ch·ªçn m√¥ h√¨nh t·ªët nh·∫•t c√≥ feature importance
    best_tree_models = ['Random Forest (Baseline)', 'XGBoost (Tuned)', 'XGBoost (Baseline)']
    
    fig, axes = plt.subplots(1, len(best_tree_models), figsize=(6*len(best_tree_models), 8))
    if len(best_tree_models) == 1:
        axes = [axes]
    
    for i, model_name in enumerate(best_tree_models):
        if model_name in models:
            model = models[model_name]
            
            # L·∫•y feature importance
            if hasattr(model, 'feature_importances_'):
                importances = model.feature_importances_
            elif hasattr(model, 'get_feature_importance'):
                importances = model.get_feature_importance()
            else:
                continue
            
            # S·∫Øp x·∫øp
            indices = np.argsort(importances)[::-1][:top_n]
            
            # V·∫Ω bi·ªÉu ƒë·ªì
            axes[i].barh(range(len(indices)), importances[indices])
            axes[i].set_yticks(range(len(indices)))
            axes[i].set_yticklabels([feature_names[idx] for idx in indices])
            axes[i].set_xlabel('Importance')
            axes[i].set_title(f'Feature Importance - {model_name}', fontsize=12, fontweight='bold')
            axes[i].invert_yaxis()
            axes[i].grid(True, alpha=0.3, axis='x')
    
    plt.tight_layout()
    output_path = os.path.join(OUTPUT_VISUALIZATIONS_DIR, 'feature_importance_analysis.png')
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"‚úì ƒê√£ l∆∞u bi·ªÉu ƒë·ªì: {output_path}")
    plt.show()

def analyze_residuals(models, X_test, y_test, weights=None, top_n=2):
    """Ph√¢n t√≠ch residuals c·ªßa top N m√¥ h√¨nh"""
    print(f"\n=== PH√ÇN T√çCH RESIDUALS (Top {top_n} m√¥ h√¨nh) ===")
    
    # T√≠nh WMAE cho t·∫•t c·∫£ m√¥ h√¨nh ƒë·ªÉ ch·ªçn top N
    metrics = create_evaluation_metrics()
    if weights is None:
        weights = np.ones(len(y_test))
    
    model_scores = []
    for model_name, model in models.items():
        y_pred = model.predict(X_test)
        wmae = metrics['wmae'](y_test, y_pred, weights)
        model_scores.append((model_name, wmae))
    
    model_scores.sort(key=lambda x: x[1])
    top_models = model_scores[:top_n]
    
    fig, axes = plt.subplots(2, top_n, figsize=(8*top_n, 12))
    if top_n == 1:
        axes = axes.reshape(2, 1)
    
    for i, (model_name, _) in enumerate(top_models):
        model = models[model_name]
        y_pred = model.predict(X_test)
        residuals = y_test - y_pred
        
        # Residuals vs Predicted
        axes[0, i].scatter(y_pred, residuals, alpha=0.3, s=1)
        axes[0, i].axhline(y=0, color='r', linestyle='--', linewidth=2)
        axes[0, i].set_xlabel('Predicted Values', fontsize=12)
        axes[0, i].set_ylabel('Residuals', fontsize=12)
        axes[0, i].set_title(f'Residuals vs Predicted - {model_name}', fontsize=12, fontweight='bold')
        axes[0, i].grid(True, alpha=0.3)
        
        # Histogram of residuals
        axes[1, i].hist(residuals, bins=50, alpha=0.7, edgecolor='black')
        axes[1, i].set_xlabel('Residuals', fontsize=12)
        axes[1, i].set_ylabel('Frequency', fontsize=12)
        axes[1, i].set_title(f'Distribution of Residuals - {model_name}', fontsize=12, fontweight='bold')
        axes[1, i].grid(True, alpha=0.3, axis='y')
        
        # Th√™m th·ªëng k√™
        mean_residual = residuals.mean()
        std_residual = residuals.std()
        axes[1, i].axvline(mean_residual, color='r', linestyle='--', linewidth=2, label=f'Mean: {mean_residual:.2f}')
        axes[1, i].legend()
    
    plt.tight_layout()
    output_path = os.path.join(OUTPUT_VISUALIZATIONS_DIR, 'residual_analysis.png')
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"‚úì ƒê√£ l∆∞u bi·ªÉu ƒë·ªì: {output_path}")
    plt.show()

def create_final_report(comparison_df, best_model):
    """T·∫°o b√°o c√°o cu·ªëi c√πng"""
    print("\n=== T·∫†O B√ÅO C√ÅO CU·ªêI C√ôNG ===")
    
    from datetime import datetime
    
    # B·∫Øt ƒë·∫ßu t·∫°o report
    report_lines = [
        "# B√ÅO C√ÅO CU·ªêI C√ôNG - WALMART SALES FORECASTING",
        "",
        f"**Ng√†y t·∫°o:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        "",
        "## T·ªîNG QUAN",
        "",
        "D·ª± √°n d·ª± b√°o doanh s·ªë Walmart s·ª≠ d·ª•ng Machine Learning truy·ªÅn th·ªëng (KH√îNG s·ª≠ d·ª•ng Deep Learning).",
        "",
        "## K·∫æT QU·∫¢ C√ÅC M√î H√åNH",
        "",
        "```",
        comparison_df.to_string(index=False),
        "```",
        "",
        "## M√î H√åNH T·ªêT NH·∫§T",
        "",
        f"**Model:** {best_model['Model']}",
        f"- **WMAE:** {best_model['WMAE']:.2f} (Metric ch√≠nh c·ªßa cu·ªôc thi)",
        f"- **MAE:** {best_model['MAE']:.2f}",
        f"- **RMSE:** {best_model['RMSE']:.2f}",
        f"- **R¬≤:** {best_model['R¬≤']:.4f}",
        f"- **MAPE:** {best_model['MAPE (%)']:.2f}%",
        "",
        "## PH√ÇN T√çCH K·∫æT QU·∫¢",
        "",
        "### 1. Baseline Models"
    ]
    
    # Th√™m th√¥ng tin baseline models
    try:
        lr_row = comparison_df[comparison_df['Model'].str.contains('Linear Regression', case=False, na=False)]
        if not lr_row.empty:
            report_lines.extend([
                f"- **Linear Regression:** WMAE = {lr_row['WMAE'].iloc[0]:.2f}",
                "  - M√¥ h√¨nh c∆° s·ªü, hi·ªáu su·∫•t th·∫•p do kh√¥ng n·∫Øm b·∫Øt ƒë∆∞·ª£c m·ªëi quan h·ªá phi tuy·∫øn",
                ""
            ])
    except:
        pass
    
    try:
        rf_baseline = comparison_df[comparison_df['Model'].str.contains('Random Forest.*Baseline', case=False, na=False, regex=True)]
        if not rf_baseline.empty:
            report_lines.extend([
                f"- **Random Forest (Baseline):** WMAE = {rf_baseline['WMAE'].iloc[0]:.2f}",
                "  - Hi·ªáu su·∫•t t·ªët, kh√¥ng c·∫ßn tuning",
                ""
            ])
    except:
        pass
    
    try:
        xgb_baseline = comparison_df[comparison_df['Model'].str.contains('XGBoost.*Baseline', case=False, na=False, regex=True)]
        if not xgb_baseline.empty:
            report_lines.extend([
                f"- **XGBoost (Baseline):** WMAE = {xgb_baseline['WMAE'].iloc[0]:.2f}",
                "  - Hi·ªáu su·∫•t t·ªët, c√≥ th·ªÉ c·∫£i thi·ªán b·∫±ng tuning",
                ""
            ])
    except:
        pass
    
    report_lines.extend([
        "### 2. Tuned Models"
    ])
    
    try:
        rf_tuned = comparison_df[comparison_df['Model'].str.contains('Random Forest.*Tuned', case=False, na=False, regex=True)]
        if not rf_tuned.empty:
            report_lines.extend([
                f"- **Random Forest (Tuned):** WMAE = {rf_tuned['WMAE'].iloc[0]:.2f}",
                "  - ‚ö†Ô∏è T·ªìi h∆°n baseline (c√≥ th·ªÉ do overfitting tr√™n validation set)",
                "  - **Khuy·∫øn ngh·ªã:** S·ª≠ d·ª•ng Random Forest Baseline thay v√¨ Tuned",
                ""
            ])
    except:
        pass
    
    try:
        xgb_tuned = comparison_df[comparison_df['Model'].str.contains('XGBoost.*Tuned', case=False, na=False, regex=True)]
        xgb_baseline = comparison_df[comparison_df['Model'].str.contains('XGBoost.*Baseline', case=False, na=False, regex=True)]
        if not xgb_tuned.empty and not xgb_baseline.empty:
            improvement = ((xgb_baseline['WMAE'].iloc[0] - xgb_tuned['WMAE'].iloc[0]) / xgb_baseline['WMAE'].iloc[0] * 100)
            report_lines.extend([
                f"- **XGBoost (Tuned):** WMAE = {xgb_tuned['WMAE'].iloc[0]:.2f}",
                f"  - ‚úÖ C·∫£i thi·ªán {improvement:.2f}% so v·ªõi baseline",
                "  - **Khuy·∫øn ngh·ªã:** S·ª≠ d·ª•ng XGBoost Tuned",
                ""
            ])
    except:
        pass
    
    report_lines.extend([
        "## KHUY·∫æN NGH·ªä",
        "",
        "### Cho Production:",
        f"1. **S·ª≠ d·ª•ng m√¥ h√¨nh:** {best_model['Model']}",
        f"2. **WMAE ƒë·∫°t ƒë∆∞·ª£c:** {best_model['WMAE']:.2f}",
        "3. **Monitor performance:** Theo d√µi WMAE tr√™n d·ªØ li·ªáu m·ªõi",
        "4. **Retrain ƒë·ªãnh k·ª≥:** C·∫≠p nh·∫≠t m√¥ h√¨nh v·ªõi d·ªØ li·ªáu m·ªõi",
        "",
        "### L∆∞u √Ω:",
        "- Random Forest Baseline t·ªët h∆°n Random Forest Tuned",
        "- XGBoost Tuned c·∫£i thi·ªán ƒë√°ng k·ªÉ so v·ªõi baseline",
        "- C√≥ th·ªÉ th·ª≠ ensemble c·ªßa Random Forest Baseline v√† XGBoost Tuned",
        "",
        "## K·∫æT LU·∫¨N",
        "",
        f"D·ª± √°n ƒë√£ th√†nh c√¥ng trong vi·ªác x√¢y d·ª±ng c√°c m√¥ h√¨nh d·ª± b√°o doanh s·ªë Walmart v·ªõi hi·ªáu su·∫•t cao.",
        f"M√¥ h√¨nh t·ªët nh·∫•t ƒë·∫°t ƒë∆∞·ª£c WMAE = {best_model['WMAE']:.2f}, cho th·∫•y kh·∫£ nƒÉng d·ª± b√°o ch√≠nh x√°c v√† ƒë√°ng tin c·∫≠y.",
        "",
        "Vi·ªác s·ª≠ d·ª•ng Time Series Cross-Validation v√† t·∫≠p trung v√†o WMAE metric ƒë·∫£m b·∫£o",
        "m√¥ h√¨nh s·∫Ω ho·∫°t ƒë·ªông t·ªët trong th·ª±c t·∫ø."
    ])
    
    report = "\n".join(report_lines)
    
    output_path = os.path.join(OUTPUT_REPORTS_DIR, 'final_report.md')
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"‚úì ƒê√£ l∆∞u b√°o c√°o: {output_path}")
    return report

def main():
    """H√†m ch√≠nh ƒë·ªÉ ch·∫°y B∆Ø·ªöC 4"""
    print("="*80)
    print("B∆Ø·ªöC 4: MODEL EVALUATION & ANALYSIS")
    print("="*80)
    
    # Load t·∫•t c·∫£ m√¥ h√¨nh
    models = load_all_models()
    if not models:
        print("\n‚ùå Kh√¥ng t√¨m th·∫•y m√¥ h√¨nh n√†o")
        return None
    
    # Load d·ªØ li·ªáu test
    X_test, y_test, weights, feature_names = load_prepared_data()
    if X_test is None:
        print("\n‚ùå Kh√¥ng th·ªÉ ti·∫øp t·ª•c do thi·∫øu d·ªØ li·ªáu")
        return None
    
    # ƒê√°nh gi√° t·∫•t c·∫£ m√¥ h√¨nh
    metrics = create_evaluation_metrics()
    results = evaluate_all_models(models, X_test, y_test, weights, metrics)
    
    # T·∫°o b·∫£ng so s√°nh cu·ªëi c√πng
    comparison_df, best_model = create_final_comparison(results)
    
    # Ph√¢n t√≠ch feature importance
    analyze_feature_importance(models, feature_names)
    
    # Ph√¢n t√≠ch residuals
    analyze_residuals(models, X_test, y_test, weights, top_n=2)
    
    # T·∫°o b√°o c√°o cu·ªëi c√πng
    final_report = create_final_report(comparison_df, best_model)
    
    # L∆∞u k·∫øt qu·∫£
    comparison_path = os.path.join(OUTPUT_REPORTS_DIR, 'final_model_comparison.csv')
    comparison_df.to_csv(comparison_path, index=False)
    print(f"\n‚úì ƒê√£ l∆∞u: {comparison_path}")
    
    # L∆∞u best model
    best_model_name = best_model['Model']
    if best_model_name in models:
        best_model_path = os.path.join(MODELS_DIR, 'best_model.pkl')
        with open(best_model_path, 'wb') as f:
            pickle.dump(models[best_model_name], f)
        print(f"‚úì ƒê√£ l∆∞u best model: {best_model_path} ({best_model_name})")
    
    print("\n" + "="*80)
    print("‚úì HO√ÄN TH√ÄNH B∆Ø·ªöC 4: MODEL EVALUATION & ANALYSIS")
    print("="*80)
    print("\nC√°c file ƒë√£ ƒë∆∞·ª£c t·∫°o:")
    print("  - final_model_comparison.csv")
    print("  - final_report.md")
    print("  - feature_importance_analysis.png")
    print("  - residual_analysis.png")
    print("  - best_model.pkl")
    print("\nüéâ HO√ÄN TH√ÄNH D·ª∞ √ÅN!")
    
    return {
        'models': models,
        'results': results,
        'comparison': comparison_df,
        'best_model': best_model,
        'report': final_report
    }

if __name__ == "__main__":
    results = main()
